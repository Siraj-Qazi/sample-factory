{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Sample Factory \u00b6 Codebase for high throughput synchronous and asynchronous reinforcement learning. Resources: Documentation: https://samplefactory.dev Paper: https://arxiv.org/abs/2006.11751 Citation: BibTeX ViZDoom, IsaacGym, DMLab-30, Megaverse, Mujoco, and Atari agents trained with Sample Factory: When should I use Sample Factory? \u00b6 Sample Factory is the fastest open source single-machine RL implementations (see paper for details). If you plan to train RL agents on large amounts of experience, consider using it. Sample Factory can significantly speed up the experimentation or allow you to collect more samples in the same amount of time and achieve better performance. Consider using Sample Factory for your multi-agent and population-based training experiments. Multi-agent and PBT setups are really simple with Sample Factory. A lot of work went into our VizDoom and DMLab wrappers. For example, we include full support for configurable VizDoom multi-agent environments and their interop with RL algorithms, which can open new interesting research directions. Consider using Sample Factory if you train agents in these environments. Sample Factory can be a good choice as a prototype for a single node in a distributed RL system or as a reference codebase for other types of async RL algorithms.","title":"Overview"},{"location":"#sample-factory","text":"Codebase for high throughput synchronous and asynchronous reinforcement learning. Resources: Documentation: https://samplefactory.dev Paper: https://arxiv.org/abs/2006.11751 Citation: BibTeX ViZDoom, IsaacGym, DMLab-30, Megaverse, Mujoco, and Atari agents trained with Sample Factory:","title":"Sample Factory"},{"location":"#when-should-i-use-sample-factory","text":"Sample Factory is the fastest open source single-machine RL implementations (see paper for details). If you plan to train RL agents on large amounts of experience, consider using it. Sample Factory can significantly speed up the experimentation or allow you to collect more samples in the same amount of time and achieve better performance. Consider using Sample Factory for your multi-agent and population-based training experiments. Multi-agent and PBT setups are really simple with Sample Factory. A lot of work went into our VizDoom and DMLab wrappers. For example, we include full support for configurable VizDoom multi-agent environments and their interop with RL algorithms, which can open new interesting research directions. Consider using Sample Factory if you train agents in these environments. Sample Factory can be a good choice as a prototype for a single node in a distributed RL system or as a reference codebase for other types of async RL algorithms.","title":"When should I use Sample Factory?"},{"location":"community/citation/","text":"Citation \u00b6 If you use this repository in your work or otherwise wish to cite it, please make reference to our ICML2020 paper. @inproceedings{petrenko2020sf, title={Sample Factory: Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning}, author={Petrenko, Aleksei and Huang, Zhehui and Kumar, Tushar and Sukhatme, Gaurav and Koltun, Vladlen}, booktitle={ICML}, year={2020} } For questions, issues, inquiries please email apetrenko1991@gmail.com . Github issues and pull requests are welcome.","title":"Citation"},{"location":"community/citation/#citation","text":"If you use this repository in your work or otherwise wish to cite it, please make reference to our ICML2020 paper. @inproceedings{petrenko2020sf, title={Sample Factory: Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning}, author={Petrenko, Aleksei and Huang, Zhehui and Kumar, Tushar and Sukhatme, Gaurav and Koltun, Vladlen}, booktitle={ICML}, year={2020} } For questions, issues, inquiries please email apetrenko1991@gmail.com . Github issues and pull requests are welcome.","title":"Citation"},{"location":"community/contribution/","text":"Contribute to SF \u00b6 How to contribute to Sample Factory? \u00b6 Sample Factory is an open source project, so all contributions and suggestions are welcome. You can contribute in many different ways: giving ideas, answering questions, reporting bugs, proposing enhancements, improving the documentation, fixing bugs,... Many thanks in advance to every contributor. How to work on an open Issue? \u00b6 You have the list of open Issues at: https://github.com/alex-petrenko/sample-factory/issues Some of them may have the label help wanted : that means that any contributor is welcomed! If you would like to work on any of the open Issues: Make sure it is not already assigned to someone else. You have the assignee (if any) on the top of the right column of the Issue page. You can self-assign it by commenting on the Issue page with one of the keywords: #take or #self-assign . Work on your self-assigned issue and eventually create a Pull Request. How to create a Pull Request? \u00b6 Fork the repository by clicking on the 'Fork' button on the repository's page. This creates a copy of the code under your GitHub user account. Clone your fork to your local disk, and add the base repository as a remote: git clone git@github.com:<your Github handle>/sample-factory.git cd sample-factory git remote add upstream https://github.com/alex-petrenko/sample-factory.git Create a new branch to hold your development changes: git checkout -b a-descriptive-name-for-my-changes do not work on the main branch. Set up a development environment by running the following command in a virtual (or conda) environment: pip install -e . [ dev ] (If sample-factory was already installed in the virtual environment, remove it with pip uninstall sample-factory before reinstalling it in editable mode with the -e flag.) Develop the features on your branch. Format your code. Run black and isort so that your newly added files look nice with the following command: make format If you want to enable auto format checking before every commit, you can run the following command: pre-commit install Run unittests with the following command: make test Once you're happy with your files, add your changes and make a commit to record your changes locally: git add sample-factory/<your_dataset_name> git commit It is a good idea to sync your copy of the code with the original repository regularly. This way you can quickly account for changes: git fetch upstream git rebase upstream/main Push the changes to your account using: git push -u origin a-descriptive-name-for-my-changes Once you are satisfied, go the webpage of your fork on GitHub. Click on \"Pull request\" to send your to the project maintainers for review.","title":"Contribute to SF"},{"location":"community/contribution/#contribute-to-sf","text":"","title":"Contribute to SF"},{"location":"community/contribution/#how-to-contribute-to-sample-factory","text":"Sample Factory is an open source project, so all contributions and suggestions are welcome. You can contribute in many different ways: giving ideas, answering questions, reporting bugs, proposing enhancements, improving the documentation, fixing bugs,... Many thanks in advance to every contributor.","title":"How to contribute to Sample Factory?"},{"location":"community/contribution/#how-to-work-on-an-open-issue","text":"You have the list of open Issues at: https://github.com/alex-petrenko/sample-factory/issues Some of them may have the label help wanted : that means that any contributor is welcomed! If you would like to work on any of the open Issues: Make sure it is not already assigned to someone else. You have the assignee (if any) on the top of the right column of the Issue page. You can self-assign it by commenting on the Issue page with one of the keywords: #take or #self-assign . Work on your self-assigned issue and eventually create a Pull Request.","title":"How to work on an open Issue?"},{"location":"community/contribution/#how-to-create-a-pull-request","text":"Fork the repository by clicking on the 'Fork' button on the repository's page. This creates a copy of the code under your GitHub user account. Clone your fork to your local disk, and add the base repository as a remote: git clone git@github.com:<your Github handle>/sample-factory.git cd sample-factory git remote add upstream https://github.com/alex-petrenko/sample-factory.git Create a new branch to hold your development changes: git checkout -b a-descriptive-name-for-my-changes do not work on the main branch. Set up a development environment by running the following command in a virtual (or conda) environment: pip install -e . [ dev ] (If sample-factory was already installed in the virtual environment, remove it with pip uninstall sample-factory before reinstalling it in editable mode with the -e flag.) Develop the features on your branch. Format your code. Run black and isort so that your newly added files look nice with the following command: make format If you want to enable auto format checking before every commit, you can run the following command: pre-commit install Run unittests with the following command: make test Once you're happy with your files, add your changes and make a commit to record your changes locally: git add sample-factory/<your_dataset_name> git commit It is a good idea to sync your copy of the code with the original repository regularly. This way you can quickly account for changes: git fetch upstream git rebase upstream/main Push the changes to your account using: git push -u origin a-descriptive-name-for-my-changes Once you are satisfied, go the webpage of your fork on GitHub. Click on \"Pull request\" to send your to the project maintainers for review.","title":"How to create a Pull Request?"},{"location":"community/doc-contribution/","text":"Doc Contribution \u00b6 workflows \u00b6 clone the target repo It should contain a \u2018docs\u2019 folder, a \u2018mkdocs.yml\u2019 config file, a \u2018docs.yml\u2019 github actions file. install common dependencies pip install mkdocs-material mkdocs-minify-plugin mkdocs-redirects mkdocs-git-revision-date-localized-plugin mkdocs-git-committers-plugin-2 mkdocs-git-authors-plugin serve the website locally mkdocs serve you should see the website on your localhost port now. modify or create markdown files modify / create your markdown files in \u2018docs\u2019 folder. add your markdown path in the \u2018nav\u2019 section of \u2018mkdocs.yml\u2019(at the bottom). Example folder-yml correspondence: commit and push your changes to remote repo. github actions will automatically push your changes to your github pages website.","title":"Doc Contribution"},{"location":"community/doc-contribution/#doc-contribution","text":"","title":"Doc Contribution"},{"location":"community/doc-contribution/#workflows","text":"clone the target repo It should contain a \u2018docs\u2019 folder, a \u2018mkdocs.yml\u2019 config file, a \u2018docs.yml\u2019 github actions file. install common dependencies pip install mkdocs-material mkdocs-minify-plugin mkdocs-redirects mkdocs-git-revision-date-localized-plugin mkdocs-git-committers-plugin-2 mkdocs-git-authors-plugin serve the website locally mkdocs serve you should see the website on your localhost port now. modify or create markdown files modify / create your markdown files in \u2018docs\u2019 folder. add your markdown path in the \u2018nav\u2019 section of \u2018mkdocs.yml\u2019(at the bottom). Example folder-yml correspondence: commit and push your changes to remote repo. github actions will automatically push your changes to your github pages website.","title":"workflows"},{"location":"environment-integrations/atari/","text":"Atari \u00b6 Installation \u00b6 Install Sample-Factory with Atari dependencies with PyPI: pip install sample-factory[atari] Running Experiments \u00b6 Run Atari experiments with the scripts in sf_examples.atari . The default parameters have been chosen to match CleanRL's configuration (see reports below) and are not tuned for throughput. TODO: provide parameters that result in faster training. To train a model in the BreakoutNoFrameskip-v4 environment: python -m sf_examples.atari.train_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" To visualize the training results, use the enjoy_atari script: python -m sf_examples.atari.enjoy_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" Multiple experiments can be run in parallel with the launcher module. atari_envs is an example launcher script that runs atari envs with 4 seeds. python -m sample_factory.launcher.run --run=sf_examples.atari.experiments.atari_envs --backend=processes --max_parallel=8 --pause_between=1 --experiments_per_gpu=10000 --num_gpus=1 List of Supported Environments \u00b6 Specify the environment to run with the --env command line parameter. The following Atari v4 environments are supported out of the box. Various APPO models trained on Atari environments are uploaded to the HuggingFace Hub. The models have all been trained for 2 billion steps with 3 seeds per experiment. Videos of the agents after training can be found on the HuggingFace Hub. Atari Command Line Parameter Atari Environment name Model Checkpooints atari_alien AlienNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_amidar AmidarNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_assault AssaultNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_asterix AsterixNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_asteroid AsteroidsNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_atlantis AtlantisNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_bankheist BankHeistNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_battlezone BattleZoneNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_beamrider BeamRiderNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_berzerk BerzerkNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_bowling BowlingNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_boxing BoxingNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_breakout BreakoutNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_centipede CentipedeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_choppercommand ChopperCommandNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_crazyclimber CrazyClimberNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_defender DefenderNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_demonattack DemonAttackNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_doubledunk DoubleDunkNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_enduro EnduroNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_fishingderby FishingDerbyNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_freeway FreewayNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_frostbite FrostbiteNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_gopher GopherNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_gravitar GravitarNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_hero HeroNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_icehockey IceHockeyNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_jamesbond JamesbondNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_kangaroo KangarooNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_krull KrullNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_kongfumaster KungFuMasterNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_montezuma MontezumaRevengeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_mspacman MsPacmanNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_namethisgame NameThisGameNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_phoenix PhoenixNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_pitfall PitfallNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_pong PongNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_privateye PrivateEyeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_qbert QbertNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_riverraid RiverraidNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_roadrunner RoadRunnerNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_robotank RobotankNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_seaquest SeaquestNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_skiing SkiingNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_solaris SolarisNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_spaceinvaders SpaceInvadersNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_stargunner StarGunnerNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_surround SurroundNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_tennis TennisNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_timepilot TimePilotNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_tutankham TutankhamNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_upndown UpNDownNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_venture VentureNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_videopinball VideoPinballNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_wizardofwor WizardOfWorNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_yarsrevenge YarsRevengeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_zaxxon ZaxxonNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints Reports \u00b6 Sample-Factory was benchmarked on Atari against CleanRL and Baselines. Sample-Factory was able to achieve similar sample efficiency as CleanRL and Baselines using the same parameters. https://wandb.ai/wmfrank/atari-benchmark/reports/Atari-Sample-Factory2-Baselines-CleanRL--VmlldzoyMzEyNjIw","title":"Atari"},{"location":"environment-integrations/atari/#atari","text":"","title":"Atari"},{"location":"environment-integrations/atari/#installation","text":"Install Sample-Factory with Atari dependencies with PyPI: pip install sample-factory[atari]","title":"Installation"},{"location":"environment-integrations/atari/#running-experiments","text":"Run Atari experiments with the scripts in sf_examples.atari . The default parameters have been chosen to match CleanRL's configuration (see reports below) and are not tuned for throughput. TODO: provide parameters that result in faster training. To train a model in the BreakoutNoFrameskip-v4 environment: python -m sf_examples.atari.train_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" To visualize the training results, use the enjoy_atari script: python -m sf_examples.atari.enjoy_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" Multiple experiments can be run in parallel with the launcher module. atari_envs is an example launcher script that runs atari envs with 4 seeds. python -m sample_factory.launcher.run --run=sf_examples.atari.experiments.atari_envs --backend=processes --max_parallel=8 --pause_between=1 --experiments_per_gpu=10000 --num_gpus=1","title":"Running Experiments"},{"location":"environment-integrations/atari/#list-of-supported-environments","text":"Specify the environment to run with the --env command line parameter. The following Atari v4 environments are supported out of the box. Various APPO models trained on Atari environments are uploaded to the HuggingFace Hub. The models have all been trained for 2 billion steps with 3 seeds per experiment. Videos of the agents after training can be found on the HuggingFace Hub. Atari Command Line Parameter Atari Environment name Model Checkpooints atari_alien AlienNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_amidar AmidarNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_assault AssaultNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_asterix AsterixNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_asteroid AsteroidsNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_atlantis AtlantisNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_bankheist BankHeistNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_battlezone BattleZoneNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_beamrider BeamRiderNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_berzerk BerzerkNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_bowling BowlingNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_boxing BoxingNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_breakout BreakoutNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_centipede CentipedeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_choppercommand ChopperCommandNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_crazyclimber CrazyClimberNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_defender DefenderNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_demonattack DemonAttackNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_doubledunk DoubleDunkNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_enduro EnduroNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_fishingderby FishingDerbyNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_freeway FreewayNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_frostbite FrostbiteNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_gopher GopherNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_gravitar GravitarNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_hero HeroNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_icehockey IceHockeyNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_jamesbond JamesbondNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_kangaroo KangarooNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_krull KrullNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_kongfumaster KungFuMasterNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_montezuma MontezumaRevengeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_mspacman MsPacmanNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_namethisgame NameThisGameNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_phoenix PhoenixNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_pitfall PitfallNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_pong PongNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_privateye PrivateEyeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_qbert QbertNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_riverraid RiverraidNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_roadrunner RoadRunnerNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_robotank RobotankNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_seaquest SeaquestNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_skiing SkiingNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_solaris SolarisNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_spaceinvaders SpaceInvadersNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_stargunner StarGunnerNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_surround SurroundNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_tennis TennisNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_timepilot TimePilotNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_tutankham TutankhamNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_upndown UpNDownNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_venture VentureNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_videopinball VideoPinballNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_wizardofwor WizardOfWorNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_yarsrevenge YarsRevengeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_zaxxon ZaxxonNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints","title":"List of Supported Environments"},{"location":"environment-integrations/atari/#reports","text":"Sample-Factory was benchmarked on Atari against CleanRL and Baselines. Sample-Factory was able to achieve similar sample efficiency as CleanRL and Baselines using the same parameters. https://wandb.ai/wmfrank/atari-benchmark/reports/Atari-Sample-Factory2-Baselines-CleanRL--VmlldzoyMzEyNjIw","title":"Reports"},{"location":"environment-integrations/dmlab/","text":"DeepMind Lab \u00b6 Installation \u00b6 Installation DeepMind Lab can be time consuming. If you are on a Linux system, we provide a prebuilt wheel . Either pip install deepmind_lab-1.0-py3-none-any.whl Or alternatively, DMLab can be compiled from source by following the instructions on the DMLab Github . pip install dm_env To train on DMLab-30 you will need brady_konkle_oliva2008 dataset . To significantly speed up training on DMLab-30 consider downloading our dataset of pre-generated environment layouts (see paper for details). Command lines for running experiments with these datasets are provided in the sections below. Running Experiments \u00b6 Run DMLab experiments with the scripts in sf_examples.dmlab . Example of training in the DMLab watermaze environment for 1B environment steps python -m sf_examples.dmlab.train_dmlab --env=doom_battle --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --batch_size=2048 --wide_aspect_ratio=False --num_workers=20 --num_envs_per_worker=20 --num_policies=1 --experiment=doom_battle_w20_v20 DMLab-30 run on a 36-core server with 4 GPUs: python -m sf_examples.dmlab.train_dmlab --env=dmlab_30 --train_for_env_steps=10000000000 --algo=APPO --gamma=0.99 --use_rnn=True --num_workers=90 --num_envs_per_worker=12 --num_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --benchmark=False --max_grad_norm=0.0 --dmlab_renderer=software --decorrelate_experience_max_seconds=120 --reset_timeout_seconds=300 --encoder_conv_architecture=resnet_impala --encoder_conv_mlp_layers=512 --nonlinearity=relu --rnn_type=lstm --dmlab_extended_action_set=True --num_policies=4 --pbt_replace_reward_gap=0.05 --pbt_replace_reward_gap_absolute=5.0 --pbt_period_env_steps=10000000 --pbt_start_mutation=100000000 --with_pbt=True --experiment=dmlab_30_resnet_4pbt_w90_v12 --dmlab_one_task_per_worker=True --set_workers_cpu_affinity=True --max_policy_lag=35 --pbt_target_objective=dmlab_target_objective --dmlab30_dataset=~/datasets/brady_konkle_oliva2008 --dmlab_use_level_cache=True --dmlab_level_cache_path=/home/user/dmlab_cache Models \u00b6 The models below are the best models from the input normalization experiment above. The evaluation metrics here are obtained by running the experiment 10 times with different seeds. DMLab Command Line Parameter DMLab Environment name Model Checkpooints dmlab_30 DMLab-30 \ud83e\udd17 Hub DMLab30 checkpoints","title":"DeepMind Lab"},{"location":"environment-integrations/dmlab/#deepmind-lab","text":"","title":"DeepMind Lab"},{"location":"environment-integrations/dmlab/#installation","text":"Installation DeepMind Lab can be time consuming. If you are on a Linux system, we provide a prebuilt wheel . Either pip install deepmind_lab-1.0-py3-none-any.whl Or alternatively, DMLab can be compiled from source by following the instructions on the DMLab Github . pip install dm_env To train on DMLab-30 you will need brady_konkle_oliva2008 dataset . To significantly speed up training on DMLab-30 consider downloading our dataset of pre-generated environment layouts (see paper for details). Command lines for running experiments with these datasets are provided in the sections below.","title":"Installation"},{"location":"environment-integrations/dmlab/#running-experiments","text":"Run DMLab experiments with the scripts in sf_examples.dmlab . Example of training in the DMLab watermaze environment for 1B environment steps python -m sf_examples.dmlab.train_dmlab --env=doom_battle --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --batch_size=2048 --wide_aspect_ratio=False --num_workers=20 --num_envs_per_worker=20 --num_policies=1 --experiment=doom_battle_w20_v20 DMLab-30 run on a 36-core server with 4 GPUs: python -m sf_examples.dmlab.train_dmlab --env=dmlab_30 --train_for_env_steps=10000000000 --algo=APPO --gamma=0.99 --use_rnn=True --num_workers=90 --num_envs_per_worker=12 --num_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --benchmark=False --max_grad_norm=0.0 --dmlab_renderer=software --decorrelate_experience_max_seconds=120 --reset_timeout_seconds=300 --encoder_conv_architecture=resnet_impala --encoder_conv_mlp_layers=512 --nonlinearity=relu --rnn_type=lstm --dmlab_extended_action_set=True --num_policies=4 --pbt_replace_reward_gap=0.05 --pbt_replace_reward_gap_absolute=5.0 --pbt_period_env_steps=10000000 --pbt_start_mutation=100000000 --with_pbt=True --experiment=dmlab_30_resnet_4pbt_w90_v12 --dmlab_one_task_per_worker=True --set_workers_cpu_affinity=True --max_policy_lag=35 --pbt_target_objective=dmlab_target_objective --dmlab30_dataset=~/datasets/brady_konkle_oliva2008 --dmlab_use_level_cache=True --dmlab_level_cache_path=/home/user/dmlab_cache","title":"Running Experiments"},{"location":"environment-integrations/dmlab/#models","text":"The models below are the best models from the input normalization experiment above. The evaluation metrics here are obtained by running the experiment 10 times with different seeds. DMLab Command Line Parameter DMLab Environment name Model Checkpooints dmlab_30 DMLab-30 \ud83e\udd17 Hub DMLab30 checkpoints","title":"Models"},{"location":"environment-integrations/envpool/","text":"Envpool \u00b6 Installation \u00b6 Install Sample-Factory with Envpool dependencies with PyPI: pip install sample-factory[atari,envpool] pip install sample-factory[mujoco,envpool] Running Experiments \u00b6 EnvPool is a C++-based batched environment pool with pybind11 and thread pool. It has high performance (~1M raw FPS with Atari games, ~3M raw FPS with Mujoco simulator. We provide examples for envpool for Atari and Mujoco environments. The default parameters provide reasonable training speed, but can be tuning based on your machine configuration to achieve higher throughput. To train a model with envpool in the BreakoutNoFrameskip-v4 environment: python -m sf_examples.envpool.atari.train_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" To visualize the training results, use the enjoy_atari script: python -m sf_examples.atari.enjoy_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" Multiple experiments can be run in parallel with the launcher module. atari_envs is an example launcher script that runs atari envs with 4 seeds. python -m sample_factory.launcher.run --run=sf_examples.envpool.atari.experiments.atari_envs --backend=processes --max_parallel=8 --pause_between=1 --experiments_per_gpu=10000 --num_gpus=1","title":"Envpool"},{"location":"environment-integrations/envpool/#envpool","text":"","title":"Envpool"},{"location":"environment-integrations/envpool/#installation","text":"Install Sample-Factory with Envpool dependencies with PyPI: pip install sample-factory[atari,envpool] pip install sample-factory[mujoco,envpool]","title":"Installation"},{"location":"environment-integrations/envpool/#running-experiments","text":"EnvPool is a C++-based batched environment pool with pybind11 and thread pool. It has high performance (~1M raw FPS with Atari games, ~3M raw FPS with Mujoco simulator. We provide examples for envpool for Atari and Mujoco environments. The default parameters provide reasonable training speed, but can be tuning based on your machine configuration to achieve higher throughput. To train a model with envpool in the BreakoutNoFrameskip-v4 environment: python -m sf_examples.envpool.atari.train_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" To visualize the training results, use the enjoy_atari script: python -m sf_examples.atari.enjoy_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" Multiple experiments can be run in parallel with the launcher module. atari_envs is an example launcher script that runs atari envs with 4 seeds. python -m sample_factory.launcher.run --run=sf_examples.envpool.atari.experiments.atari_envs --backend=processes --max_parallel=8 --pause_between=1 --experiments_per_gpu=10000 --num_gpus=1","title":"Running Experiments"},{"location":"environment-integrations/megaverse/","text":"Megaverse \u00b6 Megaverse is a dedicated high-throughput RL environment with batched GPU rendering. This document demonstrates an example of external integration, i.e. another project using Sample Factory as a library. Very likely this is going to be the most common integration scenario. Installation \u00b6 Install Megaverse according to the readme of the repo Megaverse . Further instructions assume that you are in a Python (or Conda) environment with a working Megaverse installation. Running Experiments \u00b6 Run Megaverse experiments with the scripts in megaverse_rl . To train a model in the TowerBuilding environment: python -m megaverse_rl.train_megaverse --train_for_seconds=360000000 --train_for_env_steps=2000000000 --algo=APPO --gamma=0.997 --use_rnn=True --rnn_num_layers=2 --num_workers=12 --num_envs_per_worker=2 --num_epochs=1 --rollout=32 --recurrence=32 --batch_size=4096 --actor_worker_gpus 0 --env_gpu_observations=False --num_policies=1 --with_pbt=False --max_grad_norm=0.0 --exploration_loss=symmetric_kl --exploration_loss_coeff=0.001 --megaverse_num_simulation_threads=1 --megaverse_num_envs_per_instance=32 --megaverse_num_agents_per_env=1 --megaverse_use_vulkan=True --policy_workers_per_policy=2 --reward_clip=30 --env=TowerBuilding --experiment=TowerBuilding To visualize the training results, use the enjoy_megaverse script: python -m megaverse_rl.enjoy_megaverse --algo=APPO --env=TowerBuilding --experiment=TowerBuilding --megaverse_num_envs_per_instance=1 --fps=20 --megaverse_use_vulkan=True Multiple experiments can be run in parallel with the launcher module. megaverse_envs is an example launcher script that runs atari envs with 4 seeds. python -m sample_factory.launcher.run --run=megaverse_rl.runs.single_agent --backend=processes --max_parallel=2 --pause_between=1 --experiments_per_gpu=2 --num_gpus=1 Or you could run experiments on slurm: python -m sample_factory.launcher.run --run=megaverse_rl.runs.single_agent --backend=slurm --slurm_workdir=./slurm_megaverse --experiment_suffix=slurm --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=16 --slurm_sbatch_template=./sample_factory/launcher/slurm/sbatch_timeout.sh --pause_between=1 --slurm_print_only=False Results \u00b6 Reports \u00b6 We trained models in the TowerBuilding environment in SF2 with single agent per env. https://wandb.ai/wmfrank/megaverse-benchmark/reports/Megaverse-trained-Sample-Factory--VmlldzoyNTAxMDUz Models \u00b6 An example APPO model trained on Megaverse environments is uploaded to the HuggingFace Hub. The models have all been trained for 2G steps. Environment HuggingFace Hub Models TowerBuilding https://huggingface.co/wmFrank/sample-factory-2-megaverse Videos \u00b6 Tower Building with single agent \u00b6 Tower Building with four agents \u00b6","title":"Megaverse"},{"location":"environment-integrations/megaverse/#megaverse","text":"Megaverse is a dedicated high-throughput RL environment with batched GPU rendering. This document demonstrates an example of external integration, i.e. another project using Sample Factory as a library. Very likely this is going to be the most common integration scenario.","title":"Megaverse"},{"location":"environment-integrations/megaverse/#installation","text":"Install Megaverse according to the readme of the repo Megaverse . Further instructions assume that you are in a Python (or Conda) environment with a working Megaverse installation.","title":"Installation"},{"location":"environment-integrations/megaverse/#running-experiments","text":"Run Megaverse experiments with the scripts in megaverse_rl . To train a model in the TowerBuilding environment: python -m megaverse_rl.train_megaverse --train_for_seconds=360000000 --train_for_env_steps=2000000000 --algo=APPO --gamma=0.997 --use_rnn=True --rnn_num_layers=2 --num_workers=12 --num_envs_per_worker=2 --num_epochs=1 --rollout=32 --recurrence=32 --batch_size=4096 --actor_worker_gpus 0 --env_gpu_observations=False --num_policies=1 --with_pbt=False --max_grad_norm=0.0 --exploration_loss=symmetric_kl --exploration_loss_coeff=0.001 --megaverse_num_simulation_threads=1 --megaverse_num_envs_per_instance=32 --megaverse_num_agents_per_env=1 --megaverse_use_vulkan=True --policy_workers_per_policy=2 --reward_clip=30 --env=TowerBuilding --experiment=TowerBuilding To visualize the training results, use the enjoy_megaverse script: python -m megaverse_rl.enjoy_megaverse --algo=APPO --env=TowerBuilding --experiment=TowerBuilding --megaverse_num_envs_per_instance=1 --fps=20 --megaverse_use_vulkan=True Multiple experiments can be run in parallel with the launcher module. megaverse_envs is an example launcher script that runs atari envs with 4 seeds. python -m sample_factory.launcher.run --run=megaverse_rl.runs.single_agent --backend=processes --max_parallel=2 --pause_between=1 --experiments_per_gpu=2 --num_gpus=1 Or you could run experiments on slurm: python -m sample_factory.launcher.run --run=megaverse_rl.runs.single_agent --backend=slurm --slurm_workdir=./slurm_megaverse --experiment_suffix=slurm --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=16 --slurm_sbatch_template=./sample_factory/launcher/slurm/sbatch_timeout.sh --pause_between=1 --slurm_print_only=False","title":"Running Experiments"},{"location":"environment-integrations/megaverse/#results","text":"","title":"Results"},{"location":"environment-integrations/megaverse/#reports","text":"We trained models in the TowerBuilding environment in SF2 with single agent per env. https://wandb.ai/wmfrank/megaverse-benchmark/reports/Megaverse-trained-Sample-Factory--VmlldzoyNTAxMDUz","title":"Reports"},{"location":"environment-integrations/megaverse/#models","text":"An example APPO model trained on Megaverse environments is uploaded to the HuggingFace Hub. The models have all been trained for 2G steps. Environment HuggingFace Hub Models TowerBuilding https://huggingface.co/wmFrank/sample-factory-2-megaverse","title":"Models"},{"location":"environment-integrations/megaverse/#videos","text":"","title":"Videos"},{"location":"environment-integrations/megaverse/#tower-building-with-single-agent","text":"","title":"Tower Building with single agent"},{"location":"environment-integrations/megaverse/#tower-building-with-four-agents","text":"","title":"Tower Building with four agents"},{"location":"environment-integrations/mujoco/","text":"MuJoCo \u00b6 Installation \u00b6 Install Sample-Factory with MuJoCo dependencies with PyPI: pip install sample-factory[mujoco] Running Experiments \u00b6 Run MuJoCo experiments with the scripts in sf_examples.mujoco . The default parameters have been chosen to match CleanRL's results in the report below. To train a model in the Ant-v4 enviornment: python -m sf_examples.mujoco.train_mujoco --algo=APPO --env=mujoco_ant --experiment=<experiment_name> To visualize the training results, use the enjoy_mujoco script: python -m sf_examples.mujoco.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<experiment_name> Multiple experiments can be run in parallel with the launcher module. mujoco_all_envs is an example launcher script that runs all mujoco envs with 10 seeds. python -m sample_factory.launcher.run --run=sf_examples.mujoco.experiments.mujoco_all_envs --backend=processes --max_parallel=4 --pause_between=1 --experiments_per_gpu=10000 --num_gpus=1 --experiment_suffix=0 List of Supported Environments \u00b6 Specify the environment to run with the --env command line parameter. The following MuJoCo v4 environments are supported out of the box, and more enviornments can be added as needed in sf_examples.mujoco.mujoco.mujoco_utils MuJoCo Environment Name Sample-Factory Command Line Parameter Ant-v4 mujoco_ant HalfCheetah-v4 mujoco_halfcheetah Hopper-v4 mujoco_hopper Humanoid-v4 mujoco_humanoid Walker2d-v4 mujoco_walker InvertedDoublePendulum-v4 mujoco_doublependulum InvertedPendulum-v4 mujoco_pendulum Reacher-v4 mujoco_reacher Swimmer-v4 mujoco_swimmer Results \u00b6 Reports \u00b6 Sample-Factory was benchmarked on MuJoCo against CleanRL. Sample-Factory was able to achieve similar sample efficiency as CleanRL using the same parameters. https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Sample-Factory-vs-CleanRL-w-o-EnvPool--VmlldzoyMjMyMTQ0 Sample-Factory can run experiments synchronously or asynchronously, with asynchronous execution usually having worse sample efficiency but runs faster. MuJoCo's environments were compared using the two modes in Sample-Factory https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Synchronous-vs-Asynchronous--VmlldzoyMzEzNDUz Sample-Factory comparison with CleanRL in terms of wall time. Both experiments are run on a 16 core machine with 1 GPU. Sample-Factory was able to complete 10M samples 5 times as fast as CleanRL https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Sample-Factory-vs-CleanRL-Wall-Time--VmlldzoyMzg2MDA3 Models \u00b6 Various APPO models trained on MuJoCo environments are uploaded to the HuggingFace Hub. The models have all been trained for 10M steps. Videos of the agents after training can be found on the HuggingFace Hub. The models below are the best models from the experiment against CleanRL above. The evaluation metrics here are obtained by running the model 10 times. Environment HuggingFace Hub Models Evaluation Metrics Ant-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-ant 5876.09 \u00b1 166.99 HalfCheetah-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-halfcheetah 6262.56 \u00b1 67.29 Humanoid-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-humanoid 5439.48 \u00b1 1314.24 Walker2d-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-walker 5487.74 \u00b1 48.96 Hopper-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-hopper 2793.44 \u00b1 642.58 InvertedDoublePendulum-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-doublependulum 9350.13 \u00b1 1.31 InvertedPendulum-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-pendulum 1000.00 \u00b1 0.00 Reacher-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-reacher -4.53 \u00b1 1.79 Swimmer-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-swimmer 117.28 \u00b1 2.91 Videos \u00b6 Below are some video examples of agents in various MuJoCo envioronments. Videos for all enviornments can be found in the HuggingFace Hub pages linked above. HalfCheetah-v4 \u00b6 Ant-v4 \u00b6 InvertedDoublePendulum-v4 \u00b6","title":"MuJoCo"},{"location":"environment-integrations/mujoco/#mujoco","text":"","title":"MuJoCo"},{"location":"environment-integrations/mujoco/#installation","text":"Install Sample-Factory with MuJoCo dependencies with PyPI: pip install sample-factory[mujoco]","title":"Installation"},{"location":"environment-integrations/mujoco/#running-experiments","text":"Run MuJoCo experiments with the scripts in sf_examples.mujoco . The default parameters have been chosen to match CleanRL's results in the report below. To train a model in the Ant-v4 enviornment: python -m sf_examples.mujoco.train_mujoco --algo=APPO --env=mujoco_ant --experiment=<experiment_name> To visualize the training results, use the enjoy_mujoco script: python -m sf_examples.mujoco.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<experiment_name> Multiple experiments can be run in parallel with the launcher module. mujoco_all_envs is an example launcher script that runs all mujoco envs with 10 seeds. python -m sample_factory.launcher.run --run=sf_examples.mujoco.experiments.mujoco_all_envs --backend=processes --max_parallel=4 --pause_between=1 --experiments_per_gpu=10000 --num_gpus=1 --experiment_suffix=0","title":"Running Experiments"},{"location":"environment-integrations/mujoco/#list-of-supported-environments","text":"Specify the environment to run with the --env command line parameter. The following MuJoCo v4 environments are supported out of the box, and more enviornments can be added as needed in sf_examples.mujoco.mujoco.mujoco_utils MuJoCo Environment Name Sample-Factory Command Line Parameter Ant-v4 mujoco_ant HalfCheetah-v4 mujoco_halfcheetah Hopper-v4 mujoco_hopper Humanoid-v4 mujoco_humanoid Walker2d-v4 mujoco_walker InvertedDoublePendulum-v4 mujoco_doublependulum InvertedPendulum-v4 mujoco_pendulum Reacher-v4 mujoco_reacher Swimmer-v4 mujoco_swimmer","title":"List of Supported Environments"},{"location":"environment-integrations/mujoco/#results","text":"","title":"Results"},{"location":"environment-integrations/mujoco/#reports","text":"Sample-Factory was benchmarked on MuJoCo against CleanRL. Sample-Factory was able to achieve similar sample efficiency as CleanRL using the same parameters. https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Sample-Factory-vs-CleanRL-w-o-EnvPool--VmlldzoyMjMyMTQ0 Sample-Factory can run experiments synchronously or asynchronously, with asynchronous execution usually having worse sample efficiency but runs faster. MuJoCo's environments were compared using the two modes in Sample-Factory https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Synchronous-vs-Asynchronous--VmlldzoyMzEzNDUz Sample-Factory comparison with CleanRL in terms of wall time. Both experiments are run on a 16 core machine with 1 GPU. Sample-Factory was able to complete 10M samples 5 times as fast as CleanRL https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Sample-Factory-vs-CleanRL-Wall-Time--VmlldzoyMzg2MDA3","title":"Reports"},{"location":"environment-integrations/mujoco/#models","text":"Various APPO models trained on MuJoCo environments are uploaded to the HuggingFace Hub. The models have all been trained for 10M steps. Videos of the agents after training can be found on the HuggingFace Hub. The models below are the best models from the experiment against CleanRL above. The evaluation metrics here are obtained by running the model 10 times. Environment HuggingFace Hub Models Evaluation Metrics Ant-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-ant 5876.09 \u00b1 166.99 HalfCheetah-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-halfcheetah 6262.56 \u00b1 67.29 Humanoid-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-humanoid 5439.48 \u00b1 1314.24 Walker2d-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-walker 5487.74 \u00b1 48.96 Hopper-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-hopper 2793.44 \u00b1 642.58 InvertedDoublePendulum-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-doublependulum 9350.13 \u00b1 1.31 InvertedPendulum-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-pendulum 1000.00 \u00b1 0.00 Reacher-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-reacher -4.53 \u00b1 1.79 Swimmer-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-swimmer 117.28 \u00b1 2.91","title":"Models"},{"location":"environment-integrations/mujoco/#videos","text":"Below are some video examples of agents in various MuJoCo envioronments. Videos for all enviornments can be found in the HuggingFace Hub pages linked above.","title":"Videos"},{"location":"environment-integrations/mujoco/#halfcheetah-v4","text":"","title":"HalfCheetah-v4"},{"location":"environment-integrations/mujoco/#ant-v4","text":"","title":"Ant-v4"},{"location":"environment-integrations/mujoco/#inverteddoublependulum-v4","text":"","title":"InvertedDoublePendulum-v4"},{"location":"environment-integrations/vizdoom/","text":"VizDoom \u00b6 Installation \u00b6 To install VizDoom just follow system setup instructions from the original repository ( VizDoom linux_deps ), after which the latest VizDoom can be installed from PyPI: pip install vizdoom Running Experiments \u00b6 Run MuJoCo experiments with the scripts in sf_examples.vizdoom . Train for 4B env steps (also can be stopped at any time with Ctrl+C and resumed by using the same cmd). This is more or less optimal training setup for a 10-core machine. python -m sf_examples.vizdoom.train_vizdoom --env=doom_battle --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --batch_size=2048 --wide_aspect_ratio=False --num_workers=20 --num_envs_per_worker=20 --num_policies=1 --experiment=doom_battle_w20_v20 Run at any point to visualize the experiment: python -m sf_examples.vizdoom.enjoy_vizdoom --env=doom_battle --algo=APPO --experiment=doom_battle_w20_v20 Launcher scripts are also provided in sf_examples.vizdoom.experiments to run experiments in parallel or on slurm. Reproducing Paper Results \u00b6 Train on one of the 6 \"basic\" VizDoom environments: python -m sf_examples.vizdoom.train_vizdoom --train_for_env_steps=500000000 --algo=APPO --env=doom_my_way_home --env_frameskip=4 --use_rnn=True --num_workers=36 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --experiment=doom_basic_envs Doom \"battle\" and \"battle2\" environments, 36-core server (72 logical cores) with 4 GPUs: python -m sf_examples.vizdoom.train_vizdoom --env=doom_battle --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --max_grad_norm=0.0 --experiment=doom_battle python -m sf_examples.vizdoom.train_vizdoom --env=doom_battle2 --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --max_grad_norm=0.0 --experiment=doom_battle_2 Duel and deathmatch versus bots, population-based training, 36-core server: python -m sf_examples.vizdoom.train_vizdoom --env=doom_duel_bots --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --reward_scale=0.5 --num_workers=72 --num_envs_per_worker=32 --num_policies=8 --batch_size=2048 --benchmark=False --res_w=128 --res_h=72 --wide_aspect_ratio=False --pbt_replace_reward_gap=0.2 --pbt_replace_reward_gap_absolute=3.0 --pbt_period_env_steps=5000000 --save_milestones_sec=1800 --with_pbt=True --experiment=doom_duel_bots python -m sf_examples.vizdoom.train_vizdoom --env=doom_deathmatch_bots --train_for_seconds=3600000 --algo=APPO --use_rnn=True --gamma=0.995 --env_frameskip=2 --num_workers=80 --num_envs_per_worker=24 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --with_pbt=True --pbt_period_env_steps=5000000 --experiment=doom_deathmatch_bots Duel and deathmatch self-play, PBT, 36-core server: python -m sf_examples.vizdoom.train_vizdoom --env=doom_duel --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --num_workers=72 --num_envs_per_worker=16 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --benchmark=False --pbt_replace_reward_gap=0.5 --pbt_replace_reward_gap_absolute=0.35 --pbt_period_env_steps=5000000 --with_pbt=True --pbt_start_mutation=100000000 --experiment=doom_duel_full python -m sf_examples.vizdoom.train_vizdoom --env=doom_deathmatch_full --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --num_workers=72 --num_envs_per_worker=16 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --benchmark=False --pbt_replace_reward_gap=0.1 --pbt_replace_reward_gap_absolute=0.1 --pbt_period_env_steps=5000000 --with_pbt=True --pbt_start_mutation=100000000 --experiment=doom_deathmatch_full Reproducing benchmarking results: This achieves 50K+ framerate on a 10-core machine (Intel Core i9-7900X): python -m sf_examples.vizdoom.train_vizdoom --env=doom_benchmark --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=20 --num_envs_per_worker=32 --num_policies=1 --batch_size=4096 --experiment=doom_battle_appo_fps_20_32 --res_w=128 --res_h=72 --wide_aspect_ratio=False --policy_workers_per_policy=2 --worker_num_splits=2 This achieves 100K+ framerate on a 36-core machine: python -m sf_examples.vizdoom.train_vizdoom --env=doom_benchmark --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=24 --num_policies=1 --batch_size=8192 --wide_aspect_ratio=False --experiment=doom_battle_appo_w72_v24 --policy_workers_per_policy=2 Results \u00b6 Reports \u00b6 We reproduced the paper results in SF2 in the Battle and Battle2 and compared the results using input normalization. Input normalization has improved results in the Battle environment. This experiment with input normalization was run with sf_examples.vizdoom.experiments.sf2_doom_battle_envs . Note that normalize_input=True is set compared to the results from the paper https://wandb.ai/andrewzhang505/sample_factory/reports/VizDoom-Battle-Environments--VmlldzoyMzcyODQx In SF2's bot environments (deathmatch_bots and duel_bots), we trained the agents against randomly generated bots as opposed to a curriculum of increasing bot difficulty. This is because the ViZDoom environment no longer provides the bots used in the curriculum, and SF2 no longer requires the curriculum to train properly. However, due to the differences in bot difficulty, the current training results are no longer comparable to the paper. An example training curve on deathmatch_bots with the same parameters as in the paper is shown below: https://wandb.ai/andrewzhang505/sample_factory/reports/ViZDoom-Deathmatch-Bots--VmlldzoyNzY2NDI1 Models \u00b6 The models below are the best models from the input normalization experiment above. The evaluation metrics here are obtained by running the model 10 times. Environment HuggingFace Hub Models Evaluation Metrics Battle https://huggingface.co/andrewzhang505/sample-factory-2-doom-battle 59.37 \u00b1 3.93 Battle2 https://huggingface.co/andrewzhang505/sample-factory-2-doom-battle2 36.40 \u00b1 4.20 Videos \u00b6 Doom Battle \u00b6 Doom Battle2 \u00b6","title":"VizDoom"},{"location":"environment-integrations/vizdoom/#vizdoom","text":"","title":"VizDoom"},{"location":"environment-integrations/vizdoom/#installation","text":"To install VizDoom just follow system setup instructions from the original repository ( VizDoom linux_deps ), after which the latest VizDoom can be installed from PyPI: pip install vizdoom","title":"Installation"},{"location":"environment-integrations/vizdoom/#running-experiments","text":"Run MuJoCo experiments with the scripts in sf_examples.vizdoom . Train for 4B env steps (also can be stopped at any time with Ctrl+C and resumed by using the same cmd). This is more or less optimal training setup for a 10-core machine. python -m sf_examples.vizdoom.train_vizdoom --env=doom_battle --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --batch_size=2048 --wide_aspect_ratio=False --num_workers=20 --num_envs_per_worker=20 --num_policies=1 --experiment=doom_battle_w20_v20 Run at any point to visualize the experiment: python -m sf_examples.vizdoom.enjoy_vizdoom --env=doom_battle --algo=APPO --experiment=doom_battle_w20_v20 Launcher scripts are also provided in sf_examples.vizdoom.experiments to run experiments in parallel or on slurm.","title":"Running Experiments"},{"location":"environment-integrations/vizdoom/#reproducing-paper-results","text":"Train on one of the 6 \"basic\" VizDoom environments: python -m sf_examples.vizdoom.train_vizdoom --train_for_env_steps=500000000 --algo=APPO --env=doom_my_way_home --env_frameskip=4 --use_rnn=True --num_workers=36 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --experiment=doom_basic_envs Doom \"battle\" and \"battle2\" environments, 36-core server (72 logical cores) with 4 GPUs: python -m sf_examples.vizdoom.train_vizdoom --env=doom_battle --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --max_grad_norm=0.0 --experiment=doom_battle python -m sf_examples.vizdoom.train_vizdoom --env=doom_battle2 --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --max_grad_norm=0.0 --experiment=doom_battle_2 Duel and deathmatch versus bots, population-based training, 36-core server: python -m sf_examples.vizdoom.train_vizdoom --env=doom_duel_bots --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --reward_scale=0.5 --num_workers=72 --num_envs_per_worker=32 --num_policies=8 --batch_size=2048 --benchmark=False --res_w=128 --res_h=72 --wide_aspect_ratio=False --pbt_replace_reward_gap=0.2 --pbt_replace_reward_gap_absolute=3.0 --pbt_period_env_steps=5000000 --save_milestones_sec=1800 --with_pbt=True --experiment=doom_duel_bots python -m sf_examples.vizdoom.train_vizdoom --env=doom_deathmatch_bots --train_for_seconds=3600000 --algo=APPO --use_rnn=True --gamma=0.995 --env_frameskip=2 --num_workers=80 --num_envs_per_worker=24 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --with_pbt=True --pbt_period_env_steps=5000000 --experiment=doom_deathmatch_bots Duel and deathmatch self-play, PBT, 36-core server: python -m sf_examples.vizdoom.train_vizdoom --env=doom_duel --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --num_workers=72 --num_envs_per_worker=16 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --benchmark=False --pbt_replace_reward_gap=0.5 --pbt_replace_reward_gap_absolute=0.35 --pbt_period_env_steps=5000000 --with_pbt=True --pbt_start_mutation=100000000 --experiment=doom_duel_full python -m sf_examples.vizdoom.train_vizdoom --env=doom_deathmatch_full --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --num_workers=72 --num_envs_per_worker=16 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --benchmark=False --pbt_replace_reward_gap=0.1 --pbt_replace_reward_gap_absolute=0.1 --pbt_period_env_steps=5000000 --with_pbt=True --pbt_start_mutation=100000000 --experiment=doom_deathmatch_full Reproducing benchmarking results: This achieves 50K+ framerate on a 10-core machine (Intel Core i9-7900X): python -m sf_examples.vizdoom.train_vizdoom --env=doom_benchmark --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=20 --num_envs_per_worker=32 --num_policies=1 --batch_size=4096 --experiment=doom_battle_appo_fps_20_32 --res_w=128 --res_h=72 --wide_aspect_ratio=False --policy_workers_per_policy=2 --worker_num_splits=2 This achieves 100K+ framerate on a 36-core machine: python -m sf_examples.vizdoom.train_vizdoom --env=doom_benchmark --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=24 --num_policies=1 --batch_size=8192 --wide_aspect_ratio=False --experiment=doom_battle_appo_w72_v24 --policy_workers_per_policy=2","title":"Reproducing Paper Results"},{"location":"environment-integrations/vizdoom/#results","text":"","title":"Results"},{"location":"environment-integrations/vizdoom/#reports","text":"We reproduced the paper results in SF2 in the Battle and Battle2 and compared the results using input normalization. Input normalization has improved results in the Battle environment. This experiment with input normalization was run with sf_examples.vizdoom.experiments.sf2_doom_battle_envs . Note that normalize_input=True is set compared to the results from the paper https://wandb.ai/andrewzhang505/sample_factory/reports/VizDoom-Battle-Environments--VmlldzoyMzcyODQx In SF2's bot environments (deathmatch_bots and duel_bots), we trained the agents against randomly generated bots as opposed to a curriculum of increasing bot difficulty. This is because the ViZDoom environment no longer provides the bots used in the curriculum, and SF2 no longer requires the curriculum to train properly. However, due to the differences in bot difficulty, the current training results are no longer comparable to the paper. An example training curve on deathmatch_bots with the same parameters as in the paper is shown below: https://wandb.ai/andrewzhang505/sample_factory/reports/ViZDoom-Deathmatch-Bots--VmlldzoyNzY2NDI1","title":"Reports"},{"location":"environment-integrations/vizdoom/#models","text":"The models below are the best models from the input normalization experiment above. The evaluation metrics here are obtained by running the model 10 times. Environment HuggingFace Hub Models Evaluation Metrics Battle https://huggingface.co/andrewzhang505/sample-factory-2-doom-battle 59.37 \u00b1 3.93 Battle2 https://huggingface.co/andrewzhang505/sample-factory-2-doom-battle2 36.40 \u00b1 4.20","title":"Models"},{"location":"environment-integrations/vizdoom/#videos","text":"","title":"Videos"},{"location":"environment-integrations/vizdoom/#doom-battle","text":"","title":"Doom Battle"},{"location":"environment-integrations/vizdoom/#doom-battle2","text":"","title":"Doom Battle2"},{"location":"get-started/basic-usage/","text":"Basic Usage \u00b6 Using Sample Factory \u00b6 Once Sample Factpry is installed, it defines two main entry points, one for training, and one for algorithm evaluation: * sample_factory.algorithms.appo.train_appo * sample_factory.algorithms.appo.enjoy_appo Some environments, such as VizDoom, DMLab, and Atari, are added to the env registry in the default installation, so training on these environments is as simple as providing basic configuration parameters. I.e. to train and evaluate on the most basic VizDoom environment: python -m sample_factory.algorithms.appo.train_appo --env=doom_basic --algo=APPO --train_for_env_steps=3000000 --num_workers=20 --num_envs_per_worker=20 --experiment=doom_basic python -m sample_factory.algorithms.appo.enjoy_appo --env=doom_basic --algo=APPO --experiment=doom_basic Configuration \u00b6 Sample Factory experiments are configured via command line parameters. The following command will print the help message for the algorithm-environment combination: python -m sample_factory.algorithms.appo.train_appo --algo=APPO --env=doom_battle --experiment=your_experiment --help This will print the full list of parameters, their descriptions, and their default values. Replace doom_battle with a different environment name (i.e. atari_breakout ) to get information about parameters specific to this particular environment. Once the new experiment is started, a directory containing experiment-related files is created in --train_dir location (or ./train_dir in cwd if --train_dir is not passed from command line). This directory contains a file cfg.json where all the experiment parameters are saved (including those instantiated from their default values). Most default parameter values and their help strings are defined in sample_factory/algorithms/algorithm.py and sample_factory/algorithms/appo/appo.py . Besides that, additional parameters can be defined for specific families of environments. The key parameters are: --algo (required) algorithm to use, pass value APPO to train agents with fast Async PPO. --env (required) full name that uniquely identifies the environment, starting with the env family prefix (e.g. doom_ , dmlab_ or atari_ for built-in Sample Factory envs). E.g. doom_battle or atari_breakout . --experiment (required) a name that uniquely identifies the experiment. E.g. --experiment=my_experiment . If the experiment folder with the name already exists the experiment will be resumed ! Resuming experiments after a stop is the default behavior in Sample Factory. The parameters passed from command line are taken into account, unspecified parameters will be loaded from the existing experiment cfg.json file. If you want to start a new experiment, delete the old experiment folder or change the experiment name. --train_dir location for all experiments folders, defaults to ./train_dir . --num_workers defaults to number of logical cores in the system, which will give the best throughput in most scenarios. --num_envs_per_worker will greatly affect the performance. Large values (20-30) improve hardware utilization but increase memory usage and policy lag. See example command lines below to find a value that works for your system. Must be even for the double-buffered sampling to work. Disable double-buffered sampling by setting --worker_num_splits=1 to use odd number of envs per worker (e.g. 1 env per worker). (Default: 2) Configuring actor & critic architectures \u00b6 sample_factory/algorithms/algorithm.py contains parameters that allow users to customize the architectures of neural networks involved in the training process. Sample Factory includes a few popular NN architectures for RL, such as shallow convnets for Atari and VizDoom, deeper ResNet models for DMLab, MLPs for continuous control tasks. CLI parameters allow users to choose between these existing architectures, as well as specify the type of the policy core (LSTM/GRU/feed-forward), nonlinearities, etc. Consult experiment-specific cfg.json and the source code for full list of parameters. sample_factory.envs.dmlab.dmlab_model and sample_factory.envs.doom.doom_model demonstrate how to handle environment-specific additional input spaces (e.g. natural language and/or numerical vector inputs). Script sf_examples/train_custom_env_custom_model.py demonstrates how users can define a fully custom environment-specific encoder. Whenever a fully custom actor-critic architecture is required, users are welcome to override _ActorCriticBase following examples in sample_factory/algorithms/appo/model.py .","title":"Basic Usage"},{"location":"get-started/basic-usage/#basic-usage","text":"","title":"Basic Usage"},{"location":"get-started/basic-usage/#using-sample-factory","text":"Once Sample Factpry is installed, it defines two main entry points, one for training, and one for algorithm evaluation: * sample_factory.algorithms.appo.train_appo * sample_factory.algorithms.appo.enjoy_appo Some environments, such as VizDoom, DMLab, and Atari, are added to the env registry in the default installation, so training on these environments is as simple as providing basic configuration parameters. I.e. to train and evaluate on the most basic VizDoom environment: python -m sample_factory.algorithms.appo.train_appo --env=doom_basic --algo=APPO --train_for_env_steps=3000000 --num_workers=20 --num_envs_per_worker=20 --experiment=doom_basic python -m sample_factory.algorithms.appo.enjoy_appo --env=doom_basic --algo=APPO --experiment=doom_basic","title":"Using Sample Factory"},{"location":"get-started/basic-usage/#configuration","text":"Sample Factory experiments are configured via command line parameters. The following command will print the help message for the algorithm-environment combination: python -m sample_factory.algorithms.appo.train_appo --algo=APPO --env=doom_battle --experiment=your_experiment --help This will print the full list of parameters, their descriptions, and their default values. Replace doom_battle with a different environment name (i.e. atari_breakout ) to get information about parameters specific to this particular environment. Once the new experiment is started, a directory containing experiment-related files is created in --train_dir location (or ./train_dir in cwd if --train_dir is not passed from command line). This directory contains a file cfg.json where all the experiment parameters are saved (including those instantiated from their default values). Most default parameter values and their help strings are defined in sample_factory/algorithms/algorithm.py and sample_factory/algorithms/appo/appo.py . Besides that, additional parameters can be defined for specific families of environments. The key parameters are: --algo (required) algorithm to use, pass value APPO to train agents with fast Async PPO. --env (required) full name that uniquely identifies the environment, starting with the env family prefix (e.g. doom_ , dmlab_ or atari_ for built-in Sample Factory envs). E.g. doom_battle or atari_breakout . --experiment (required) a name that uniquely identifies the experiment. E.g. --experiment=my_experiment . If the experiment folder with the name already exists the experiment will be resumed ! Resuming experiments after a stop is the default behavior in Sample Factory. The parameters passed from command line are taken into account, unspecified parameters will be loaded from the existing experiment cfg.json file. If you want to start a new experiment, delete the old experiment folder or change the experiment name. --train_dir location for all experiments folders, defaults to ./train_dir . --num_workers defaults to number of logical cores in the system, which will give the best throughput in most scenarios. --num_envs_per_worker will greatly affect the performance. Large values (20-30) improve hardware utilization but increase memory usage and policy lag. See example command lines below to find a value that works for your system. Must be even for the double-buffered sampling to work. Disable double-buffered sampling by setting --worker_num_splits=1 to use odd number of envs per worker (e.g. 1 env per worker). (Default: 2)","title":"Configuration"},{"location":"get-started/basic-usage/#configuring-actor-critic-architectures","text":"sample_factory/algorithms/algorithm.py contains parameters that allow users to customize the architectures of neural networks involved in the training process. Sample Factory includes a few popular NN architectures for RL, such as shallow convnets for Atari and VizDoom, deeper ResNet models for DMLab, MLPs for continuous control tasks. CLI parameters allow users to choose between these existing architectures, as well as specify the type of the policy core (LSTM/GRU/feed-forward), nonlinearities, etc. Consult experiment-specific cfg.json and the source code for full list of parameters. sample_factory.envs.dmlab.dmlab_model and sample_factory.envs.doom.doom_model demonstrate how to handle environment-specific additional input spaces (e.g. natural language and/or numerical vector inputs). Script sf_examples/train_custom_env_custom_model.py demonstrates how users can define a fully custom environment-specific encoder. Whenever a fully custom actor-critic architecture is required, users are welcome to override _ActorCriticBase following examples in sample_factory/algorithms/appo/model.py .","title":"Configuring actor &amp; critic architectures"},{"location":"get-started/cfg-params/","text":"Configuration Parameters \u00b6 Training Parameters \u00b6 The command line arguments / config parameters for training using Sample Factory can be found by running your training script with the --help flag. The list of config parameters below was obtained from running python -m sf_examples.train_gym_env --env=CartPole-v1 --help . These params can be used in any environment. Other environments may have other custom params than can be viewed with --help usage: train_gym_env.py [-h] [--algo ALGO] --env ENV [--experiment EXPERIMENT] [--train_dir TRAIN_DIR] [--restart_behavior {resume,restart,overwrite}] [--device {gpu,cpu}] [--seed SEED] [--num_policies NUM_POLICIES] [--async_rl ASYNC_RL] [--serial_mode SERIAL_MODE] [--batched_sampling BATCHED_SAMPLING] [--num_batches_to_accumulate NUM_BATCHES_TO_ACCUMULATE] [--worker_num_splits WORKER_NUM_SPLITS] [--policy_workers_per_policy POLICY_WORKERS_PER_POLICY] [--max_policy_lag MAX_POLICY_LAG] [--num_workers NUM_WORKERS] [--num_envs_per_worker NUM_ENVS_PER_WORKER] [--batch_size BATCH_SIZE] [--num_batches_per_epoch NUM_BATCHES_PER_EPOCH] [--num_epochs NUM_EPOCHS] [--rollout ROLLOUT] [--recurrence RECURRENCE] [--shuffle_minibatches SHUFFLE_MINIBATCHES] [--gamma GAMMA] [--reward_scale REWARD_SCALE] [--reward_clip REWARD_CLIP] [--value_bootstrap VALUE_BOOTSTRAP] [--normalize_returns NORMALIZE_RETURNS] [--exploration_loss_coeff EXPLORATION_LOSS_COEFF] [--value_loss_coeff VALUE_LOSS_COEFF] [--kl_loss_coeff KL_LOSS_COEFF] [--exploration_loss {entropy,symmetric_kl}] [--gae_lambda GAE_LAMBDA] [--ppo_clip_ratio PPO_CLIP_RATIO] [--ppo_clip_value PPO_CLIP_VALUE] [--with_vtrace WITH_VTRACE] [--vtrace_rho VTRACE_RHO] [--vtrace_c VTRACE_C] [--optimizer {adam,lamb}] [--adam_eps ADAM_EPS] [--adam_beta1 ADAM_BETA1] [--adam_beta2 ADAM_BETA2] [--max_grad_norm MAX_GRAD_NORM] [--learning_rate LEARNING_RATE] [--lr_schedule {constant,kl_adaptive_minibatch,kl_adaptive_epoch}] [--lr_schedule_kl_threshold LR_SCHEDULE_KL_THRESHOLD] [--obs_subtract_mean OBS_SUBTRACT_MEAN] [--obs_scale OBS_SCALE] [--normalize_input NORMALIZE_INPUT] [--normalize_input_keys [NORMALIZE_INPUT_KEYS [NORMALIZE_INPUT_KEYS ...]]] [--decorrelate_experience_max_seconds DECORRELATE_EXPERIENCE_MAX_SECONDS] [--decorrelate_envs_on_one_worker DECORRELATE_ENVS_ON_ONE_WORKER] [--actor_worker_gpus [ACTOR_WORKER_GPUS [ACTOR_WORKER_GPUS ...]]] [--set_workers_cpu_affinity SET_WORKERS_CPU_AFFINITY] [--force_envs_single_thread FORCE_ENVS_SINGLE_THREAD] [--default_niceness DEFAULT_NICENESS] [--log_to_file LOG_TO_FILE] [--experiment_summaries_interval EXPERIMENT_SUMMARIES_INTERVAL] [--flush_summaries_interval FLUSH_SUMMARIES_INTERVAL] [--stats_avg STATS_AVG] [--summaries_use_frameskip SUMMARIES_USE_FRAMESKIP] [--heartbeat_interval HEARTBEAT_INTERVAL] [--heartbeat_reporting_interval HEARTBEAT_REPORTING_INTERVAL] [--train_for_env_steps TRAIN_FOR_ENV_STEPS] [--train_for_seconds TRAIN_FOR_SECONDS] [--save_every_sec SAVE_EVERY_SEC] [--keep_checkpoints KEEP_CHECKPOINTS] [--load_checkpoint_kind {latest,best}] [--save_milestones_sec SAVE_MILESTONES_SEC] [--save_best_every_sec SAVE_BEST_EVERY_SEC] [--save_best_metric SAVE_BEST_METRIC] [--save_best_after SAVE_BEST_AFTER] [--benchmark BENCHMARK] [--encoder_mlp_layers [ENCODER_MLP_LAYERS [ENCODER_MLP_LAYERS ...]]] [--encoder_conv_architecture {convnet_simple,convnet_impala,convnet_atari,resnet_impala}] [--encoder_conv_mlp_layers [ENCODER_CONV_MLP_LAYERS [ENCODER_CONV_MLP_LAYERS ...]]] [--use_rnn USE_RNN] [--rnn_size RNN_SIZE] [--rnn_type {gru,lstm}] [--rnn_num_layers RNN_NUM_LAYERS] [--decoder_mlp_layers [DECODER_MLP_LAYERS [DECODER_MLP_LAYERS ...]]] [--nonlinearity {elu,relu,tanh}] [--policy_initialization {orthogonal,xavier_uniform,torch_default}] [--policy_init_gain POLICY_INIT_GAIN] [--actor_critic_share_weights ACTOR_CRITIC_SHARE_WEIGHTS] [--adaptive_stddev ADAPTIVE_STDDEV] [--continuous_tanh_scale CONTINUOUS_TANH_SCALE] [--initial_stddev INITIAL_STDDEV] [--use_env_info_cache USE_ENV_INFO_CACHE] [--env_gpu_actions ENV_GPU_ACTIONS] [--env_gpu_observations ENV_GPU_OBSERVATIONS] [--env_frameskip ENV_FRAMESKIP] [--env_framestack ENV_FRAMESTACK] [--pixel_format PIXEL_FORMAT] [--use_record_episode_statistics USE_RECORD_EPISODE_STATISTICS] [--with_wandb WITH_WANDB] [--wandb_user WANDB_USER] [--wandb_project WANDB_PROJECT] [--wandb_group WANDB_GROUP] [--wandb_job_type WANDB_JOB_TYPE] [--wandb_tags [WANDB_TAGS [WANDB_TAGS ...]]] [--with_pbt WITH_PBT] [--pbt_mix_policies_in_one_env PBT_MIX_POLICIES_IN_ONE_ENV] [--pbt_period_env_steps PBT_PERIOD_ENV_STEPS] [--pbt_start_mutation PBT_START_MUTATION] [--pbt_replace_fraction PBT_REPLACE_FRACTION] [--pbt_mutation_rate PBT_MUTATION_RATE] [--pbt_replace_reward_gap PBT_REPLACE_REWARD_GAP] [--pbt_replace_reward_gap_absolute PBT_REPLACE_REWARD_GAP_ABSOLUTE] [--pbt_optimize_gamma PBT_OPTIMIZE_GAMMA] [--pbt_target_objective PBT_TARGET_OBJECTIVE] [--pbt_perturb_min PBT_PERTURB_MIN] [--pbt_perturb_max PBT_PERTURB_MAX] optional arguments: -h, --help Print the help message (default: False) --algo ALGO Algorithm to use (default: APPO) --env ENV Name of the environment to use (default: None) --experiment EXPERIMENT Unique experiment name. This will also be the name for the experiment folder in the train dir.If the experiment folder with this name aleady exists the experiment will be RESUMED!Any parameters passed from command line that do not match the parameters stored in the experiment cfg.json file will be overridden. (default: default_experiment) --train_dir TRAIN_DIR Root for all experiments (default: /home/runner/work/sample-factory/sample- factory/train_dir) --restart_behavior {resume,restart,overwrite} How to handle the experiment if the directory with the same name already exists. \"resume\" (default) will resume the experiment, \"restart\" will preserve the existing experiment folder under a different name (with \"old\" suffix) and will start training from scratch, \"overwrite\" will delete the existing experiment folder and start from scratch. This parameter does not have any effect if the experiment directory does not exist. (default: resume) --device {gpu,cpu} CPU training is only recommended for smaller e.g. MLP policies (default: gpu) --seed SEED Set a fixed seed value (default: None) --num_policies NUM_POLICIES Number of policies to train jointly, i.e. for multi- agent environments (default: 1) --async_rl ASYNC_RL Collect experience asynchronously while learning on the previous batch. This is significantly different from standard synchronous actor-critic (or PPO) because not all of the experience will be collected by the latest policy thus increasing policy lag. Negative effects of using async_rl can range from negligible (just grants you throughput boost) to quite serious where you can consider switching it off. It all depends how sensitive your experiment is to policy lag. Envs with complex action spaces and RNN policies tend to be particularly sensitive. (default: True) --serial_mode SERIAL_MODE Enable serial mode: run everything completely synchronously in the same process (default: False) --batched_sampling BATCHED_SAMPLING Batched sampling allows the data to be processed in big batches on the rollout worker.This is especially important for GPU-accelerated vectorized environments such as Megaverse or IsaacGym. As a downside, in batched mode we do not support (for now) some of the features, such as population-based self-play or inactive agents, plus each batched sampler (rollout worker) process only collects data for a single policy. Another issue between batched/non-batched sampling is handling of infos. In batched mode we assume that infos is a single dictionary of lists/tensors containing info for each environment in a vector. If you need some complex info dictionary handling and your environment might return dicts with different keys, on different rollout steps, you probably need non-batched mode. (default: False) --num_batches_to_accumulate NUM_BATCHES_TO_ACCUMULATE This parameter governs the maximum number of training batches the learner can accumulate before further experience collection is stopped. The default value will set this to 2, so if the experience collection is faster than the training, the learner will accumulate enough minibatches for 2 iterations of training but no more. This is a good balance between policy-lag and throughput. When the limit is reached, the learner will notify the actor workers that they ought to stop the experience collection until accumulated minibatches are processed. Set this parameter to 1 to further reduce policy-lag. If the experience collection is very non-uniform, increasing this parameter can increase overall throughput, at the cost of increased policy-lag. (default: 2) --worker_num_splits WORKER_NUM_SPLITS Typically we split a vector of envs into two parts for \"double buffered\" experience collection Set this to 1 to disable double buffering. Set this to 3 for triple buffering! (default: 2) --policy_workers_per_policy POLICY_WORKERS_PER_POLICY Number of policy workers that compute forward pass (per policy) (default: 1) --max_policy_lag MAX_POLICY_LAG Max policy lag in policy versions. Discard all experience that is older than this. (default: 1000) --num_workers NUM_WORKERS Number of parallel environment workers. Should be less than num_envs and should divide num_envs.Use this in async mode. (default: 2) --num_envs_per_worker NUM_ENVS_PER_WORKER Number of envs on a single CPU actor, in high- throughput configurations this should be in 10-30 range for Atari/VizDoomMust be even for double- buffered sampling! (default: 2) --batch_size BATCH_SIZE Minibatch size for SGD (default: 1024) --num_batches_per_epoch NUM_BATCHES_PER_EPOCH This determines the training dataset size for each iteration of training. We collect this many minibatches before performing any SGD. Example: if batch_size=128 and num_batches_per_epoch=2, then learner will process 2*128=256 environment transitions in one training iteration. (default: 1) --num_epochs NUM_EPOCHS Number of training epochs on a dataset of collected experiences of size batch_size x num_batches_per_epoch (default: 1) --rollout ROLLOUT Length of the rollout from each environment in timesteps.Once we collect this many timesteps on actor worker, we send this trajectory to the learner.The length of the rollout will determine how many timesteps are used to calculate bootstrappedMonte- Carlo estimates of discounted rewards, advantages, GAE, or V-trace targets. Shorter rolloutsreduce variance, but the estimates are less precise (bias vs variance tradeoff).For RNN policies, this should be a multiple of --recurrence, so every rollout will be splitinto (n = rollout / recurrence) segments for backpropagation. V-trace algorithm currently requires thatrollout == recurrence, which what you want most of the time anyway.Rollout length is independent from the episode length. Episode length can be both shorter or longer thanrollout, although for PBT training it is currently recommended that rollout << episode_len(see function finalize_trajectory in actor_worker.py) (default: 32) --recurrence RECURRENCE Trajectory length for backpropagation through time. If recurrence=1 there is no backpropagation through time, and experience is shuffled completely randomlyFor V-trace recurrence should be equal to rollout length. (default: 32) --shuffle_minibatches SHUFFLE_MINIBATCHES Whether to randomize and shuffle minibatches between iterations (this is a slow operation when batches are large, disabling this increases learner throughput when training with multiple epochs/minibatches per epoch) (default: False) --gamma GAMMA Discount factor (default: 0.99) --reward_scale REWARD_SCALE Multiply all rewards by this factor before feeding into RL algorithm.Sometimes the overall scale of rewards is too high which makes value estimation a harder regression task.Loss values become too high which requires a smaller learning rate, etc. (default: 1.0) --reward_clip REWARD_CLIP Clip rewards between [-c, c]. Default [-1000, 1000] should mean no clipping for most envs (unless rewards are very large/small) (default: 1000.0) --value_bootstrap VALUE_BOOTSTRAP Bootstrap returns from value estimates if episode is terminated by timeout. More info here: https://github.com/Denys88/rl_games/issues/128 (default: False) --normalize_returns NORMALIZE_RETURNS Whether to use running mean and standard deviation to normalize discounted returns (default: True) --exploration_loss_coeff EXPLORATION_LOSS_COEFF Coefficient for the exploration component of the loss function. (default: 0.003) --value_loss_coeff VALUE_LOSS_COEFF Coefficient for the critic loss (default: 0.5) --kl_loss_coeff KL_LOSS_COEFF Coefficient for fixed KL loss (as used by Schulman et al. in https://arxiv.org/pdf/1707.06347.pdf). Highly recommended for environments with continuous action spaces. (default: 0.0) --exploration_loss {entropy,symmetric_kl} Usually the exploration loss is based on maximizing the entropy of the probability distribution. Note that mathematically maximizing entropy of the categorical probability distribution is exactly the same as minimizing the (regular) KL-divergence between this distribution and a uniform prior. The downside of using the entropy term (or regular asymmetric KL- divergence) is the fact that penalty does not increase as probabilities of some actions approach zero. I.e. numerically, there is almost no difference between an action distribution with a probability epsilon > 0 for some action and an action distribution with a probability = zero for this action. For many tasks the first (epsilon) distribution is preferrable because we keep some (albeit small) amount of exploration, while the second distribution will never explore this action ever again.Unlike the entropy term, symmetric KL divergence between the action distribution and a uniform prior approaches infinity when entropy of the distribution approaches zero, so it can prevent the pathological situations where the agent stops exploring. Empirically, symmetric KL-divergence yielded slightly better results on some problems. (default: entropy) --gae_lambda GAE_LAMBDA Generalized Advantage Estimation discounting (only used when V-trace is False) (default: 0.95) --ppo_clip_ratio PPO_CLIP_RATIO We use unbiased clip(x, 1+e, 1/(1+e)) instead of clip(x, 1+e, 1-e) in the paper (default: 0.1) --ppo_clip_value PPO_CLIP_VALUE Maximum absolute change in value estimate until it is clipped. Sensitive to value magnitude (default: 1.0) --with_vtrace WITH_VTRACE Enables V-trace off-policy correction. If this is True, then GAE is not used (default: False) --vtrace_rho VTRACE_RHO rho_hat clipping parameter of the V-trace algorithm (importance sampling truncation) (default: 1.0) --vtrace_c VTRACE_C c_hat clipping parameter of the V-trace algorithm. Low values for c_hat can reduce variance of the advantage estimates (similar to GAE lambda < 1) (default: 1.0) --optimizer {adam,lamb} Type of optimizer to use (default: adam) --adam_eps ADAM_EPS Adam epsilon parameter (1e-8 to 1e-5 seem to reliably work okay, 1e-3 and up does not work) (default: 1e-06) --adam_beta1 ADAM_BETA1 Adam momentum decay coefficient (default: 0.9) --adam_beta2 ADAM_BETA2 Adam second momentum decay coefficient (default: 0.999) --max_grad_norm MAX_GRAD_NORM Max L2 norm of the gradient vector, set to 0 to disable gradient clipping (default: 4.0) --learning_rate LEARNING_RATE LR (default: 0.0001) --lr_schedule {constant,kl_adaptive_minibatch,kl_adaptive_epoch} Learning rate schedule to use. Constant keeps constant learning rate throughout training.kl_adaptive* schedulers look at --lr_schedule_kl_threshold and if KL-divergence with behavior policyafter the last minibatch/epoch significantly deviates from this threshold, lr is apropriatelyincreased or decreased (default: constant) --lr_schedule_kl_threshold LR_SCHEDULE_KL_THRESHOLD Used with kl_adaptive_* schedulers (default: 0.008) --obs_subtract_mean OBS_SUBTRACT_MEAN Observation preprocessing, mean value to subtract from observation (e.g. 128.0 for 8-bit RGB) (default: 0.0) --obs_scale OBS_SCALE Observation preprocessing, divide observation tensors by this scalar (e.g. 128.0 for 8-bit RGB) (default: 1.0) --normalize_input NORMALIZE_INPUT Whether to use running mean and standard deviation to normalize observations (default: True) --normalize_input_keys [NORMALIZE_INPUT_KEYS [NORMALIZE_INPUT_KEYS ...]] Which observation keys to use for normalization. If None, all observation keys are used (be careful with this!) (default: None) --decorrelate_experience_max_seconds DECORRELATE_EXPERIENCE_MAX_SECONDS Decorrelating experience serves two benefits. First: this is better for learning because samples from workers come from random moments in the episode, becoming more \"i.i.d\".Second, and more important one: this is good for environments with highly non-uniform one-step times, including long and expensive episode resets. If experience is not decorrelatedthen training batches will come in bursts e.g. after a bunch of environments finished resets and many iterations on the learner might be required,which will increase the policy-lag of the new experience collected. The performance of the Sample Factory is best when experience is generated as more-or-lessuniform stream. Try increasing this to 100-200 seconds to smoothen the experience distribution in time right from the beginning (it will eventually spread out and settle anyways) (default: 0) --decorrelate_envs_on_one_worker DECORRELATE_ENVS_ON_ONE_WORKER In addition to temporal decorrelation of worker processes, also decorrelate envs within one worker process. For environments with a fixed episode length it can prevent the reset from happening in the same rollout for all envs simultaneously, which makes experience collection more uniform. (default: True) --actor_worker_gpus [ACTOR_WORKER_GPUS [ACTOR_WORKER_GPUS ...]] By default, actor workers only use CPUs. Changes this if e.g. you need GPU-based rendering on the actors (default: []) --set_workers_cpu_affinity SET_WORKERS_CPU_AFFINITY Whether to assign workers to specific CPU cores or not. The logic is beneficial for most workloads because prevents a lot of context switching.However for some environments it can be better to disable it, to allow one worker to use all cores some of the time. This can be the case for some DMLab environments with very expensive episode resetthat can use parallel CPU cores for level generation. (default: True) --force_envs_single_thread FORCE_ENVS_SINGLE_THREAD Some environments may themselves use parallel libraries such as OpenMP or MKL. Since we parallelize environments on the level of workers, there is no need to keep this parallel semantic.This flag uses threadpoolctl to force libraries such as OpenMP and MKL to use only a single thread within the environment.Enabling this is recommended unless you are running fewer workers than CPU cores. threadpoolctl has caused a bunch of crashes in the past, so this feature is disabled by default at this moment. (default: False) --default_niceness DEFAULT_NICENESS Niceness of the highest priority process (the learner). Values below zero require elevated privileges. (default: 0) --log_to_file LOG_TO_FILE Whether to log to a file (sf_log.txt in the experiment folder) or not. If False, logs to stdout only. It can make sense to disable this in a slow server filesystem environment like NFS. (default: True) --experiment_summaries_interval EXPERIMENT_SUMMARIES_INTERVAL How often in seconds we write avg. statistics about the experiment (reward, episode length, extra stats...) (default: 10) --flush_summaries_interval FLUSH_SUMMARIES_INTERVAL How often do we flush tensorboard summaries (set to higher value for slow NFS-based server filesystems) (default: 30) --stats_avg STATS_AVG How many episodes to average to measure performance (avg. reward etc) (default: 100) --summaries_use_frameskip SUMMARIES_USE_FRAMESKIP Whether to multiply training steps by frameskip when recording summaries, FPS, etc. When this flag is set to True, x-axis for all summaries corresponds to the total number of simulated steps, i.e. with frameskip=4 the x-axis value of 4 million will correspond to 1 million frames observed by the policy. (default: True) --heartbeat_interval HEARTBEAT_INTERVAL How often in seconds components send a heartbeat signal to the runner to verify they are not stuck (default: 10) --heartbeat_reporting_interval HEARTBEAT_REPORTING_INTERVAL How often in seconds the runner checks for heartbeats (default: 60) --train_for_env_steps TRAIN_FOR_ENV_STEPS Stop after all policies are trained for this many env steps (default: 10000000000) --train_for_seconds TRAIN_FOR_SECONDS Stop training after this many seconds (default: 10000000000) --save_every_sec SAVE_EVERY_SEC Checkpointing rate (default: 120) --keep_checkpoints KEEP_CHECKPOINTS Number of model checkpoints to keep (default: 2) --load_checkpoint_kind {latest,best} Whether to load from latest or best checkpoint (default: latest) --save_milestones_sec SAVE_MILESTONES_SEC Save intermediate checkpoints in a separate folder for later evaluation (default=never) (default: -1) --save_best_every_sec SAVE_BEST_EVERY_SEC How often we check if we should save the policy with the best score ever (default: 5) --save_best_metric SAVE_BEST_METRIC Save \"best\" policies based on this metric (just env reward by default) (default: reward) --save_best_after SAVE_BEST_AFTER Start saving \"best\" policies after this many env steps to filter lucky episodes that succeed and dominate the statistics early on (default: 100000) --benchmark BENCHMARK Benchmark mode (default: False) --encoder_mlp_layers [ENCODER_MLP_LAYERS [ENCODER_MLP_LAYERS ...]] In case of MLP encoder, sizes of layers to use. This is ignored if observations are images. (default: [512, 512]) --encoder_conv_architecture {convnet_simple,convnet_impala,convnet_atari,resnet_impala} Architecture of the convolutional encoder. See models.py for details. VizDoom and DMLab examples demonstrate how to define custom architectures. (default: convnet_simple) --encoder_conv_mlp_layers [ENCODER_CONV_MLP_LAYERS [ENCODER_CONV_MLP_LAYERS ...]] Optional fully connected layers after the convolutional encoder head. (default: [512]) --use_rnn USE_RNN Whether to use RNN core in a policy or not (default: True) --rnn_size RNN_SIZE Size of the RNN hidden state in recurrent model (e.g. GRU or LSTM) (default: 512) --rnn_type {gru,lstm} Type of RNN cell to use if use_rnn is True (default: gru) --rnn_num_layers RNN_NUM_LAYERS Number of RNN layers to use if use_rnn is True (default: 1) --decoder_mlp_layers [DECODER_MLP_LAYERS [DECODER_MLP_LAYERS ...]] Optional decoder MLP layers after the policy core. If empty (default) decoder is identity function. (default: []) --nonlinearity {elu,relu,tanh} Type of nonlinearity to use. (default: elu) --policy_initialization {orthogonal,xavier_uniform,torch_default} NN weight initialization (default: orthogonal) --policy_init_gain POLICY_INIT_GAIN Gain parameter of PyTorch initialization schemas (i.e. Xavier) (default: 1.0) --actor_critic_share_weights ACTOR_CRITIC_SHARE_WEIGHTS Whether to share the weights between policy and value function (default: True) --adaptive_stddev ADAPTIVE_STDDEV Only for continuous action distributions, whether stddev is state-dependent or just a single learned parameter (default: True) --continuous_tanh_scale CONTINUOUS_TANH_SCALE Only for continuous action distributions, whether to use tanh squashing and what scale to use. Applies tanh(mu / scale) * scale to distribution means. Experimental. Currently only works with adaptive_stddev=False (TODO). (default: 0.0) --initial_stddev INITIAL_STDDEV Initial value for non-adaptive stddev. Only makes sense for continuous action spaces (default: 1.0) --use_env_info_cache USE_ENV_INFO_CACHE Whether to use cached env info (default: False) --env_gpu_actions ENV_GPU_ACTIONS Set to true if environment expects actions on GPU (i.e. as a GPU-side PyTorch tensor) (default: False) --env_gpu_observations ENV_GPU_OBSERVATIONS Setting this to True together with non-empty --actor_worker_gpus will make observations GPU-side PyTorch tensors. Otherwise data will be on CPU. For CPU-based envs just set --actor_worker_gpus to empty list then this parameter does not matter. (default: True) --env_frameskip ENV_FRAMESKIP Number of frames for action repeat (frame skipping). Setting this to >1 will not add any wrappers that will do frame-skipping, although this can be used in the environment factory function to add these wrappers or to tell the environment itself to skip a desired number of frames i.e. as it is done in VizDoom. FPS metrics will be multiplied by the frameskip value, i.e. 100000FPS with frameskip=4 actually corresponds to 100000/4=25000 samples per second observed by the policy. Frameskip=1 (default) means no frameskip, we process every frame. (default: 1) --env_framestack ENV_FRAMESTACK Frame stacking (only used in Atari, and it is usually set to 4) (default: 1) --pixel_format PIXEL_FORMAT PyTorch expects CHW by default, Ray & TensorFlow expect HWC (default: CHW) --use_record_episode_statistics USE_RECORD_EPISODE_STATISTICS Whether to use gym RecordEpisodeStatistics wrapper to keep track of reward (default: False) --with_wandb WITH_WANDB Enables Weights and Biases integration (default: False) --wandb_user WANDB_USER WandB username (entity). Must be specified from command line! Also see https://docs.wandb.ai/quickstart#1.-set-up-wandb (default: None) --wandb_project WANDB_PROJECT WandB \"Project\" (default: sample_factory) --wandb_group WANDB_GROUP WandB \"Group\" (to group your experiments). By default this is the name of the env. (default: None) --wandb_job_type WANDB_JOB_TYPE WandB job type (default: SF) --wandb_tags [WANDB_TAGS [WANDB_TAGS ...]] Tags can help with finding experiments in WandB web console (default: []) --with_pbt WITH_PBT Enables population-based training (PBT) (default: False) --pbt_mix_policies_in_one_env PBT_MIX_POLICIES_IN_ONE_ENV For multi-agent envs, whether we mix different policies in one env. (default: True) --pbt_period_env_steps PBT_PERIOD_ENV_STEPS Periodically replace the worst policies with the best ones and perturb the hyperparameters (default: 5000000) --pbt_start_mutation PBT_START_MUTATION Allow initial diversification, start PBT after this many env steps (default: 20000000) --pbt_replace_fraction PBT_REPLACE_FRACTION A portion of policies performing worst to be replace by better policies (rounded up) (default: 0.3) --pbt_mutation_rate PBT_MUTATION_RATE Probability that a parameter mutates (default: 0.15) --pbt_replace_reward_gap PBT_REPLACE_REWARD_GAP Relative gap in true reward when replacing weights of the policy with a better performing one (default: 0.1) --pbt_replace_reward_gap_absolute PBT_REPLACE_REWARD_GAP_ABSOLUTE Absolute gap in true reward when replacing weights of the policy with a better performing one (default: 1e-06) --pbt_optimize_gamma PBT_OPTIMIZE_GAMMA Whether to optimize gamma, discount factor, or not (experimental) (default: False) --pbt_target_objective PBT_TARGET_OBJECTIVE Policy stat to optimize with PBT. true_objective (default) is equal to raw env reward if not specified, but can also be any other per-policy stat.For DMlab-30 use value \"dmlab_target_objective\" (which is capped human normalized score) (default: true_objective) --pbt_perturb_min PBT_PERTURB_MIN When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.1) --pbt_perturb_max PBT_PERTURB_MAX When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.5)","title":"Configuration Parameters"},{"location":"get-started/cfg-params/#configuration-parameters","text":"","title":"Configuration Parameters"},{"location":"get-started/cfg-params/#training-parameters","text":"The command line arguments / config parameters for training using Sample Factory can be found by running your training script with the --help flag. The list of config parameters below was obtained from running python -m sf_examples.train_gym_env --env=CartPole-v1 --help . These params can be used in any environment. Other environments may have other custom params than can be viewed with --help usage: train_gym_env.py [-h] [--algo ALGO] --env ENV [--experiment EXPERIMENT] [--train_dir TRAIN_DIR] [--restart_behavior {resume,restart,overwrite}] [--device {gpu,cpu}] [--seed SEED] [--num_policies NUM_POLICIES] [--async_rl ASYNC_RL] [--serial_mode SERIAL_MODE] [--batched_sampling BATCHED_SAMPLING] [--num_batches_to_accumulate NUM_BATCHES_TO_ACCUMULATE] [--worker_num_splits WORKER_NUM_SPLITS] [--policy_workers_per_policy POLICY_WORKERS_PER_POLICY] [--max_policy_lag MAX_POLICY_LAG] [--num_workers NUM_WORKERS] [--num_envs_per_worker NUM_ENVS_PER_WORKER] [--batch_size BATCH_SIZE] [--num_batches_per_epoch NUM_BATCHES_PER_EPOCH] [--num_epochs NUM_EPOCHS] [--rollout ROLLOUT] [--recurrence RECURRENCE] [--shuffle_minibatches SHUFFLE_MINIBATCHES] [--gamma GAMMA] [--reward_scale REWARD_SCALE] [--reward_clip REWARD_CLIP] [--value_bootstrap VALUE_BOOTSTRAP] [--normalize_returns NORMALIZE_RETURNS] [--exploration_loss_coeff EXPLORATION_LOSS_COEFF] [--value_loss_coeff VALUE_LOSS_COEFF] [--kl_loss_coeff KL_LOSS_COEFF] [--exploration_loss {entropy,symmetric_kl}] [--gae_lambda GAE_LAMBDA] [--ppo_clip_ratio PPO_CLIP_RATIO] [--ppo_clip_value PPO_CLIP_VALUE] [--with_vtrace WITH_VTRACE] [--vtrace_rho VTRACE_RHO] [--vtrace_c VTRACE_C] [--optimizer {adam,lamb}] [--adam_eps ADAM_EPS] [--adam_beta1 ADAM_BETA1] [--adam_beta2 ADAM_BETA2] [--max_grad_norm MAX_GRAD_NORM] [--learning_rate LEARNING_RATE] [--lr_schedule {constant,kl_adaptive_minibatch,kl_adaptive_epoch}] [--lr_schedule_kl_threshold LR_SCHEDULE_KL_THRESHOLD] [--obs_subtract_mean OBS_SUBTRACT_MEAN] [--obs_scale OBS_SCALE] [--normalize_input NORMALIZE_INPUT] [--normalize_input_keys [NORMALIZE_INPUT_KEYS [NORMALIZE_INPUT_KEYS ...]]] [--decorrelate_experience_max_seconds DECORRELATE_EXPERIENCE_MAX_SECONDS] [--decorrelate_envs_on_one_worker DECORRELATE_ENVS_ON_ONE_WORKER] [--actor_worker_gpus [ACTOR_WORKER_GPUS [ACTOR_WORKER_GPUS ...]]] [--set_workers_cpu_affinity SET_WORKERS_CPU_AFFINITY] [--force_envs_single_thread FORCE_ENVS_SINGLE_THREAD] [--default_niceness DEFAULT_NICENESS] [--log_to_file LOG_TO_FILE] [--experiment_summaries_interval EXPERIMENT_SUMMARIES_INTERVAL] [--flush_summaries_interval FLUSH_SUMMARIES_INTERVAL] [--stats_avg STATS_AVG] [--summaries_use_frameskip SUMMARIES_USE_FRAMESKIP] [--heartbeat_interval HEARTBEAT_INTERVAL] [--heartbeat_reporting_interval HEARTBEAT_REPORTING_INTERVAL] [--train_for_env_steps TRAIN_FOR_ENV_STEPS] [--train_for_seconds TRAIN_FOR_SECONDS] [--save_every_sec SAVE_EVERY_SEC] [--keep_checkpoints KEEP_CHECKPOINTS] [--load_checkpoint_kind {latest,best}] [--save_milestones_sec SAVE_MILESTONES_SEC] [--save_best_every_sec SAVE_BEST_EVERY_SEC] [--save_best_metric SAVE_BEST_METRIC] [--save_best_after SAVE_BEST_AFTER] [--benchmark BENCHMARK] [--encoder_mlp_layers [ENCODER_MLP_LAYERS [ENCODER_MLP_LAYERS ...]]] [--encoder_conv_architecture {convnet_simple,convnet_impala,convnet_atari,resnet_impala}] [--encoder_conv_mlp_layers [ENCODER_CONV_MLP_LAYERS [ENCODER_CONV_MLP_LAYERS ...]]] [--use_rnn USE_RNN] [--rnn_size RNN_SIZE] [--rnn_type {gru,lstm}] [--rnn_num_layers RNN_NUM_LAYERS] [--decoder_mlp_layers [DECODER_MLP_LAYERS [DECODER_MLP_LAYERS ...]]] [--nonlinearity {elu,relu,tanh}] [--policy_initialization {orthogonal,xavier_uniform,torch_default}] [--policy_init_gain POLICY_INIT_GAIN] [--actor_critic_share_weights ACTOR_CRITIC_SHARE_WEIGHTS] [--adaptive_stddev ADAPTIVE_STDDEV] [--continuous_tanh_scale CONTINUOUS_TANH_SCALE] [--initial_stddev INITIAL_STDDEV] [--use_env_info_cache USE_ENV_INFO_CACHE] [--env_gpu_actions ENV_GPU_ACTIONS] [--env_gpu_observations ENV_GPU_OBSERVATIONS] [--env_frameskip ENV_FRAMESKIP] [--env_framestack ENV_FRAMESTACK] [--pixel_format PIXEL_FORMAT] [--use_record_episode_statistics USE_RECORD_EPISODE_STATISTICS] [--with_wandb WITH_WANDB] [--wandb_user WANDB_USER] [--wandb_project WANDB_PROJECT] [--wandb_group WANDB_GROUP] [--wandb_job_type WANDB_JOB_TYPE] [--wandb_tags [WANDB_TAGS [WANDB_TAGS ...]]] [--with_pbt WITH_PBT] [--pbt_mix_policies_in_one_env PBT_MIX_POLICIES_IN_ONE_ENV] [--pbt_period_env_steps PBT_PERIOD_ENV_STEPS] [--pbt_start_mutation PBT_START_MUTATION] [--pbt_replace_fraction PBT_REPLACE_FRACTION] [--pbt_mutation_rate PBT_MUTATION_RATE] [--pbt_replace_reward_gap PBT_REPLACE_REWARD_GAP] [--pbt_replace_reward_gap_absolute PBT_REPLACE_REWARD_GAP_ABSOLUTE] [--pbt_optimize_gamma PBT_OPTIMIZE_GAMMA] [--pbt_target_objective PBT_TARGET_OBJECTIVE] [--pbt_perturb_min PBT_PERTURB_MIN] [--pbt_perturb_max PBT_PERTURB_MAX] optional arguments: -h, --help Print the help message (default: False) --algo ALGO Algorithm to use (default: APPO) --env ENV Name of the environment to use (default: None) --experiment EXPERIMENT Unique experiment name. This will also be the name for the experiment folder in the train dir.If the experiment folder with this name aleady exists the experiment will be RESUMED!Any parameters passed from command line that do not match the parameters stored in the experiment cfg.json file will be overridden. (default: default_experiment) --train_dir TRAIN_DIR Root for all experiments (default: /home/runner/work/sample-factory/sample- factory/train_dir) --restart_behavior {resume,restart,overwrite} How to handle the experiment if the directory with the same name already exists. \"resume\" (default) will resume the experiment, \"restart\" will preserve the existing experiment folder under a different name (with \"old\" suffix) and will start training from scratch, \"overwrite\" will delete the existing experiment folder and start from scratch. This parameter does not have any effect if the experiment directory does not exist. (default: resume) --device {gpu,cpu} CPU training is only recommended for smaller e.g. MLP policies (default: gpu) --seed SEED Set a fixed seed value (default: None) --num_policies NUM_POLICIES Number of policies to train jointly, i.e. for multi- agent environments (default: 1) --async_rl ASYNC_RL Collect experience asynchronously while learning on the previous batch. This is significantly different from standard synchronous actor-critic (or PPO) because not all of the experience will be collected by the latest policy thus increasing policy lag. Negative effects of using async_rl can range from negligible (just grants you throughput boost) to quite serious where you can consider switching it off. It all depends how sensitive your experiment is to policy lag. Envs with complex action spaces and RNN policies tend to be particularly sensitive. (default: True) --serial_mode SERIAL_MODE Enable serial mode: run everything completely synchronously in the same process (default: False) --batched_sampling BATCHED_SAMPLING Batched sampling allows the data to be processed in big batches on the rollout worker.This is especially important for GPU-accelerated vectorized environments such as Megaverse or IsaacGym. As a downside, in batched mode we do not support (for now) some of the features, such as population-based self-play or inactive agents, plus each batched sampler (rollout worker) process only collects data for a single policy. Another issue between batched/non-batched sampling is handling of infos. In batched mode we assume that infos is a single dictionary of lists/tensors containing info for each environment in a vector. If you need some complex info dictionary handling and your environment might return dicts with different keys, on different rollout steps, you probably need non-batched mode. (default: False) --num_batches_to_accumulate NUM_BATCHES_TO_ACCUMULATE This parameter governs the maximum number of training batches the learner can accumulate before further experience collection is stopped. The default value will set this to 2, so if the experience collection is faster than the training, the learner will accumulate enough minibatches for 2 iterations of training but no more. This is a good balance between policy-lag and throughput. When the limit is reached, the learner will notify the actor workers that they ought to stop the experience collection until accumulated minibatches are processed. Set this parameter to 1 to further reduce policy-lag. If the experience collection is very non-uniform, increasing this parameter can increase overall throughput, at the cost of increased policy-lag. (default: 2) --worker_num_splits WORKER_NUM_SPLITS Typically we split a vector of envs into two parts for \"double buffered\" experience collection Set this to 1 to disable double buffering. Set this to 3 for triple buffering! (default: 2) --policy_workers_per_policy POLICY_WORKERS_PER_POLICY Number of policy workers that compute forward pass (per policy) (default: 1) --max_policy_lag MAX_POLICY_LAG Max policy lag in policy versions. Discard all experience that is older than this. (default: 1000) --num_workers NUM_WORKERS Number of parallel environment workers. Should be less than num_envs and should divide num_envs.Use this in async mode. (default: 2) --num_envs_per_worker NUM_ENVS_PER_WORKER Number of envs on a single CPU actor, in high- throughput configurations this should be in 10-30 range for Atari/VizDoomMust be even for double- buffered sampling! (default: 2) --batch_size BATCH_SIZE Minibatch size for SGD (default: 1024) --num_batches_per_epoch NUM_BATCHES_PER_EPOCH This determines the training dataset size for each iteration of training. We collect this many minibatches before performing any SGD. Example: if batch_size=128 and num_batches_per_epoch=2, then learner will process 2*128=256 environment transitions in one training iteration. (default: 1) --num_epochs NUM_EPOCHS Number of training epochs on a dataset of collected experiences of size batch_size x num_batches_per_epoch (default: 1) --rollout ROLLOUT Length of the rollout from each environment in timesteps.Once we collect this many timesteps on actor worker, we send this trajectory to the learner.The length of the rollout will determine how many timesteps are used to calculate bootstrappedMonte- Carlo estimates of discounted rewards, advantages, GAE, or V-trace targets. Shorter rolloutsreduce variance, but the estimates are less precise (bias vs variance tradeoff).For RNN policies, this should be a multiple of --recurrence, so every rollout will be splitinto (n = rollout / recurrence) segments for backpropagation. V-trace algorithm currently requires thatrollout == recurrence, which what you want most of the time anyway.Rollout length is independent from the episode length. Episode length can be both shorter or longer thanrollout, although for PBT training it is currently recommended that rollout << episode_len(see function finalize_trajectory in actor_worker.py) (default: 32) --recurrence RECURRENCE Trajectory length for backpropagation through time. If recurrence=1 there is no backpropagation through time, and experience is shuffled completely randomlyFor V-trace recurrence should be equal to rollout length. (default: 32) --shuffle_minibatches SHUFFLE_MINIBATCHES Whether to randomize and shuffle minibatches between iterations (this is a slow operation when batches are large, disabling this increases learner throughput when training with multiple epochs/minibatches per epoch) (default: False) --gamma GAMMA Discount factor (default: 0.99) --reward_scale REWARD_SCALE Multiply all rewards by this factor before feeding into RL algorithm.Sometimes the overall scale of rewards is too high which makes value estimation a harder regression task.Loss values become too high which requires a smaller learning rate, etc. (default: 1.0) --reward_clip REWARD_CLIP Clip rewards between [-c, c]. Default [-1000, 1000] should mean no clipping for most envs (unless rewards are very large/small) (default: 1000.0) --value_bootstrap VALUE_BOOTSTRAP Bootstrap returns from value estimates if episode is terminated by timeout. More info here: https://github.com/Denys88/rl_games/issues/128 (default: False) --normalize_returns NORMALIZE_RETURNS Whether to use running mean and standard deviation to normalize discounted returns (default: True) --exploration_loss_coeff EXPLORATION_LOSS_COEFF Coefficient for the exploration component of the loss function. (default: 0.003) --value_loss_coeff VALUE_LOSS_COEFF Coefficient for the critic loss (default: 0.5) --kl_loss_coeff KL_LOSS_COEFF Coefficient for fixed KL loss (as used by Schulman et al. in https://arxiv.org/pdf/1707.06347.pdf). Highly recommended for environments with continuous action spaces. (default: 0.0) --exploration_loss {entropy,symmetric_kl} Usually the exploration loss is based on maximizing the entropy of the probability distribution. Note that mathematically maximizing entropy of the categorical probability distribution is exactly the same as minimizing the (regular) KL-divergence between this distribution and a uniform prior. The downside of using the entropy term (or regular asymmetric KL- divergence) is the fact that penalty does not increase as probabilities of some actions approach zero. I.e. numerically, there is almost no difference between an action distribution with a probability epsilon > 0 for some action and an action distribution with a probability = zero for this action. For many tasks the first (epsilon) distribution is preferrable because we keep some (albeit small) amount of exploration, while the second distribution will never explore this action ever again.Unlike the entropy term, symmetric KL divergence between the action distribution and a uniform prior approaches infinity when entropy of the distribution approaches zero, so it can prevent the pathological situations where the agent stops exploring. Empirically, symmetric KL-divergence yielded slightly better results on some problems. (default: entropy) --gae_lambda GAE_LAMBDA Generalized Advantage Estimation discounting (only used when V-trace is False) (default: 0.95) --ppo_clip_ratio PPO_CLIP_RATIO We use unbiased clip(x, 1+e, 1/(1+e)) instead of clip(x, 1+e, 1-e) in the paper (default: 0.1) --ppo_clip_value PPO_CLIP_VALUE Maximum absolute change in value estimate until it is clipped. Sensitive to value magnitude (default: 1.0) --with_vtrace WITH_VTRACE Enables V-trace off-policy correction. If this is True, then GAE is not used (default: False) --vtrace_rho VTRACE_RHO rho_hat clipping parameter of the V-trace algorithm (importance sampling truncation) (default: 1.0) --vtrace_c VTRACE_C c_hat clipping parameter of the V-trace algorithm. Low values for c_hat can reduce variance of the advantage estimates (similar to GAE lambda < 1) (default: 1.0) --optimizer {adam,lamb} Type of optimizer to use (default: adam) --adam_eps ADAM_EPS Adam epsilon parameter (1e-8 to 1e-5 seem to reliably work okay, 1e-3 and up does not work) (default: 1e-06) --adam_beta1 ADAM_BETA1 Adam momentum decay coefficient (default: 0.9) --adam_beta2 ADAM_BETA2 Adam second momentum decay coefficient (default: 0.999) --max_grad_norm MAX_GRAD_NORM Max L2 norm of the gradient vector, set to 0 to disable gradient clipping (default: 4.0) --learning_rate LEARNING_RATE LR (default: 0.0001) --lr_schedule {constant,kl_adaptive_minibatch,kl_adaptive_epoch} Learning rate schedule to use. Constant keeps constant learning rate throughout training.kl_adaptive* schedulers look at --lr_schedule_kl_threshold and if KL-divergence with behavior policyafter the last minibatch/epoch significantly deviates from this threshold, lr is apropriatelyincreased or decreased (default: constant) --lr_schedule_kl_threshold LR_SCHEDULE_KL_THRESHOLD Used with kl_adaptive_* schedulers (default: 0.008) --obs_subtract_mean OBS_SUBTRACT_MEAN Observation preprocessing, mean value to subtract from observation (e.g. 128.0 for 8-bit RGB) (default: 0.0) --obs_scale OBS_SCALE Observation preprocessing, divide observation tensors by this scalar (e.g. 128.0 for 8-bit RGB) (default: 1.0) --normalize_input NORMALIZE_INPUT Whether to use running mean and standard deviation to normalize observations (default: True) --normalize_input_keys [NORMALIZE_INPUT_KEYS [NORMALIZE_INPUT_KEYS ...]] Which observation keys to use for normalization. If None, all observation keys are used (be careful with this!) (default: None) --decorrelate_experience_max_seconds DECORRELATE_EXPERIENCE_MAX_SECONDS Decorrelating experience serves two benefits. First: this is better for learning because samples from workers come from random moments in the episode, becoming more \"i.i.d\".Second, and more important one: this is good for environments with highly non-uniform one-step times, including long and expensive episode resets. If experience is not decorrelatedthen training batches will come in bursts e.g. after a bunch of environments finished resets and many iterations on the learner might be required,which will increase the policy-lag of the new experience collected. The performance of the Sample Factory is best when experience is generated as more-or-lessuniform stream. Try increasing this to 100-200 seconds to smoothen the experience distribution in time right from the beginning (it will eventually spread out and settle anyways) (default: 0) --decorrelate_envs_on_one_worker DECORRELATE_ENVS_ON_ONE_WORKER In addition to temporal decorrelation of worker processes, also decorrelate envs within one worker process. For environments with a fixed episode length it can prevent the reset from happening in the same rollout for all envs simultaneously, which makes experience collection more uniform. (default: True) --actor_worker_gpus [ACTOR_WORKER_GPUS [ACTOR_WORKER_GPUS ...]] By default, actor workers only use CPUs. Changes this if e.g. you need GPU-based rendering on the actors (default: []) --set_workers_cpu_affinity SET_WORKERS_CPU_AFFINITY Whether to assign workers to specific CPU cores or not. The logic is beneficial for most workloads because prevents a lot of context switching.However for some environments it can be better to disable it, to allow one worker to use all cores some of the time. This can be the case for some DMLab environments with very expensive episode resetthat can use parallel CPU cores for level generation. (default: True) --force_envs_single_thread FORCE_ENVS_SINGLE_THREAD Some environments may themselves use parallel libraries such as OpenMP or MKL. Since we parallelize environments on the level of workers, there is no need to keep this parallel semantic.This flag uses threadpoolctl to force libraries such as OpenMP and MKL to use only a single thread within the environment.Enabling this is recommended unless you are running fewer workers than CPU cores. threadpoolctl has caused a bunch of crashes in the past, so this feature is disabled by default at this moment. (default: False) --default_niceness DEFAULT_NICENESS Niceness of the highest priority process (the learner). Values below zero require elevated privileges. (default: 0) --log_to_file LOG_TO_FILE Whether to log to a file (sf_log.txt in the experiment folder) or not. If False, logs to stdout only. It can make sense to disable this in a slow server filesystem environment like NFS. (default: True) --experiment_summaries_interval EXPERIMENT_SUMMARIES_INTERVAL How often in seconds we write avg. statistics about the experiment (reward, episode length, extra stats...) (default: 10) --flush_summaries_interval FLUSH_SUMMARIES_INTERVAL How often do we flush tensorboard summaries (set to higher value for slow NFS-based server filesystems) (default: 30) --stats_avg STATS_AVG How many episodes to average to measure performance (avg. reward etc) (default: 100) --summaries_use_frameskip SUMMARIES_USE_FRAMESKIP Whether to multiply training steps by frameskip when recording summaries, FPS, etc. When this flag is set to True, x-axis for all summaries corresponds to the total number of simulated steps, i.e. with frameskip=4 the x-axis value of 4 million will correspond to 1 million frames observed by the policy. (default: True) --heartbeat_interval HEARTBEAT_INTERVAL How often in seconds components send a heartbeat signal to the runner to verify they are not stuck (default: 10) --heartbeat_reporting_interval HEARTBEAT_REPORTING_INTERVAL How often in seconds the runner checks for heartbeats (default: 60) --train_for_env_steps TRAIN_FOR_ENV_STEPS Stop after all policies are trained for this many env steps (default: 10000000000) --train_for_seconds TRAIN_FOR_SECONDS Stop training after this many seconds (default: 10000000000) --save_every_sec SAVE_EVERY_SEC Checkpointing rate (default: 120) --keep_checkpoints KEEP_CHECKPOINTS Number of model checkpoints to keep (default: 2) --load_checkpoint_kind {latest,best} Whether to load from latest or best checkpoint (default: latest) --save_milestones_sec SAVE_MILESTONES_SEC Save intermediate checkpoints in a separate folder for later evaluation (default=never) (default: -1) --save_best_every_sec SAVE_BEST_EVERY_SEC How often we check if we should save the policy with the best score ever (default: 5) --save_best_metric SAVE_BEST_METRIC Save \"best\" policies based on this metric (just env reward by default) (default: reward) --save_best_after SAVE_BEST_AFTER Start saving \"best\" policies after this many env steps to filter lucky episodes that succeed and dominate the statistics early on (default: 100000) --benchmark BENCHMARK Benchmark mode (default: False) --encoder_mlp_layers [ENCODER_MLP_LAYERS [ENCODER_MLP_LAYERS ...]] In case of MLP encoder, sizes of layers to use. This is ignored if observations are images. (default: [512, 512]) --encoder_conv_architecture {convnet_simple,convnet_impala,convnet_atari,resnet_impala} Architecture of the convolutional encoder. See models.py for details. VizDoom and DMLab examples demonstrate how to define custom architectures. (default: convnet_simple) --encoder_conv_mlp_layers [ENCODER_CONV_MLP_LAYERS [ENCODER_CONV_MLP_LAYERS ...]] Optional fully connected layers after the convolutional encoder head. (default: [512]) --use_rnn USE_RNN Whether to use RNN core in a policy or not (default: True) --rnn_size RNN_SIZE Size of the RNN hidden state in recurrent model (e.g. GRU or LSTM) (default: 512) --rnn_type {gru,lstm} Type of RNN cell to use if use_rnn is True (default: gru) --rnn_num_layers RNN_NUM_LAYERS Number of RNN layers to use if use_rnn is True (default: 1) --decoder_mlp_layers [DECODER_MLP_LAYERS [DECODER_MLP_LAYERS ...]] Optional decoder MLP layers after the policy core. If empty (default) decoder is identity function. (default: []) --nonlinearity {elu,relu,tanh} Type of nonlinearity to use. (default: elu) --policy_initialization {orthogonal,xavier_uniform,torch_default} NN weight initialization (default: orthogonal) --policy_init_gain POLICY_INIT_GAIN Gain parameter of PyTorch initialization schemas (i.e. Xavier) (default: 1.0) --actor_critic_share_weights ACTOR_CRITIC_SHARE_WEIGHTS Whether to share the weights between policy and value function (default: True) --adaptive_stddev ADAPTIVE_STDDEV Only for continuous action distributions, whether stddev is state-dependent or just a single learned parameter (default: True) --continuous_tanh_scale CONTINUOUS_TANH_SCALE Only for continuous action distributions, whether to use tanh squashing and what scale to use. Applies tanh(mu / scale) * scale to distribution means. Experimental. Currently only works with adaptive_stddev=False (TODO). (default: 0.0) --initial_stddev INITIAL_STDDEV Initial value for non-adaptive stddev. Only makes sense for continuous action spaces (default: 1.0) --use_env_info_cache USE_ENV_INFO_CACHE Whether to use cached env info (default: False) --env_gpu_actions ENV_GPU_ACTIONS Set to true if environment expects actions on GPU (i.e. as a GPU-side PyTorch tensor) (default: False) --env_gpu_observations ENV_GPU_OBSERVATIONS Setting this to True together with non-empty --actor_worker_gpus will make observations GPU-side PyTorch tensors. Otherwise data will be on CPU. For CPU-based envs just set --actor_worker_gpus to empty list then this parameter does not matter. (default: True) --env_frameskip ENV_FRAMESKIP Number of frames for action repeat (frame skipping). Setting this to >1 will not add any wrappers that will do frame-skipping, although this can be used in the environment factory function to add these wrappers or to tell the environment itself to skip a desired number of frames i.e. as it is done in VizDoom. FPS metrics will be multiplied by the frameskip value, i.e. 100000FPS with frameskip=4 actually corresponds to 100000/4=25000 samples per second observed by the policy. Frameskip=1 (default) means no frameskip, we process every frame. (default: 1) --env_framestack ENV_FRAMESTACK Frame stacking (only used in Atari, and it is usually set to 4) (default: 1) --pixel_format PIXEL_FORMAT PyTorch expects CHW by default, Ray & TensorFlow expect HWC (default: CHW) --use_record_episode_statistics USE_RECORD_EPISODE_STATISTICS Whether to use gym RecordEpisodeStatistics wrapper to keep track of reward (default: False) --with_wandb WITH_WANDB Enables Weights and Biases integration (default: False) --wandb_user WANDB_USER WandB username (entity). Must be specified from command line! Also see https://docs.wandb.ai/quickstart#1.-set-up-wandb (default: None) --wandb_project WANDB_PROJECT WandB \"Project\" (default: sample_factory) --wandb_group WANDB_GROUP WandB \"Group\" (to group your experiments). By default this is the name of the env. (default: None) --wandb_job_type WANDB_JOB_TYPE WandB job type (default: SF) --wandb_tags [WANDB_TAGS [WANDB_TAGS ...]] Tags can help with finding experiments in WandB web console (default: []) --with_pbt WITH_PBT Enables population-based training (PBT) (default: False) --pbt_mix_policies_in_one_env PBT_MIX_POLICIES_IN_ONE_ENV For multi-agent envs, whether we mix different policies in one env. (default: True) --pbt_period_env_steps PBT_PERIOD_ENV_STEPS Periodically replace the worst policies with the best ones and perturb the hyperparameters (default: 5000000) --pbt_start_mutation PBT_START_MUTATION Allow initial diversification, start PBT after this many env steps (default: 20000000) --pbt_replace_fraction PBT_REPLACE_FRACTION A portion of policies performing worst to be replace by better policies (rounded up) (default: 0.3) --pbt_mutation_rate PBT_MUTATION_RATE Probability that a parameter mutates (default: 0.15) --pbt_replace_reward_gap PBT_REPLACE_REWARD_GAP Relative gap in true reward when replacing weights of the policy with a better performing one (default: 0.1) --pbt_replace_reward_gap_absolute PBT_REPLACE_REWARD_GAP_ABSOLUTE Absolute gap in true reward when replacing weights of the policy with a better performing one (default: 1e-06) --pbt_optimize_gamma PBT_OPTIMIZE_GAMMA Whether to optimize gamma, discount factor, or not (experimental) (default: False) --pbt_target_objective PBT_TARGET_OBJECTIVE Policy stat to optimize with PBT. true_objective (default) is equal to raw env reward if not specified, but can also be any other per-policy stat.For DMlab-30 use value \"dmlab_target_objective\" (which is capped human normalized score) (default: true_objective) --pbt_perturb_min PBT_PERTURB_MIN When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.1) --pbt_perturb_max PBT_PERTURB_MAX When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.5)","title":"Training Parameters"},{"location":"get-started/customizing/","text":"Customizing Sample Factory \u00b6 Custom environments \u00b6 Training agents in your own environment with sample-factory is straightforward, but if you get stuck feel free to raise an issue on our GitHub Page . We recommend look at our example environment integrations such as Atari or ViZDoom before using your own environment. Custom Env template \u00b6 We provide the following template, which you can modify to intergrate your environment. We assume your environment conforms to a gym 0.26 API (5-tuple). First make a file called train_custom_env.py and copy the following template. from sample_factory.cfg.arguments import parse_full_cfg , parse_sf_args from sample_factory.envs.env_utils import register_env from sample_factory.train import run_rl def make_custom_env ( env_name , cfg , env_config ): # function that build your custom env # cfg and env_config can be used to further customize the env # env_config.env_id can be used to seed each env, for example # env = create_your_custom_env() return env def register_custom_env_envs (): # register the env in sample-factory's global env registry register_env ( \"custom_env_name\" , make_custom_env ) def add_custom_env_args ( _env , p : argparse . ArgumentParser , evaluation = False ): # You can extend the command line arguments here p . add_argument ( \"--custom_argument\" , default = \"value\" , type = str , help = \"\" ) def custom_env_override_defaults ( _env , parser ): # Modify the default arguments when using this env # these can still be changed from the command line parser . set_defaults ( encoder_conv_architecture = \"convnet_atari\" , obs_scale = 255.0 , gamma = 0.99 , learning_rate = 0.00025 , lr_schedule = \"linear_decay\" , adam_eps = 1e-5 , ) def parse_args ( argv = None , evaluation = False ): # parse the command line arguments to build parser , partial_cfg = parse_sf_args ( argv = argv , evaluation = evaluation ) add_custom_env_args ( partial_cfg . env , parser , evaluation = evaluation ) custom_env_override_defaults ( partial_cfg . env , parser ) final_cfg = parse_full_cfg ( parser , argv ) return final_cfg def main (): \"\"\"Script entry point.\"\"\" register_custom_env_envs () cfg = parse_args () status = run_rl ( cfg ) return status if __name__ == \"__main__\" : sys . exit ( main ()) Training can now be started with python train_custom_env --env=custom_env_name Custom models \u00b6 Adding custom models in sample factory is simple, but if you get stuck feel free to raise an issue on our GitHub Page . Actor Critic models in sample factory \u00b6 Actor Critic models in Sample Factory are composed of three components: Encoders - Process input observations (images, vectors) and map them to a vector. Cores - Intergrate vectors from one or more encoders, can optionally include an LSTM in a memory-based agent. Decoders - Apply addition linear layers to the output of the model core. You can register custom versions of each component, or you can register an entire Actor Critic model. Custom model template \u00b6 from sample_factory.model.encoder import Encoder from sample_factory.model.decoder import Decoder from sample_factory.model.core import ModelCore from sample_factory.model.actor_critic import ActorCritic from sample_factory.algo.utils.context import global_model_factory class CustomEncoder ( Encoder ): def __init__ ( self , cfg : Config , obs_space : ObsSpace ): super () . __init__ ( cfg ) # build custom encoder architecture ... def forward ( self , obs_dict ): # custom forward logic ... class CustomCore ( ModelCore ): def __init__ ( self , cfg : Config , input_size : int ): super () . __init__ ( cfg ) # build custom core architecture ... def forward ( self , head_output , rnn_states ): # custom forward logic ... class CustomDecoder ( Decoder ): def __init__ ( self , cfg : Config , decoder_input_size : int ): super () . __init__ ( cfg ) # build custom decoder architecture ... def forward ( self , core_output ): # custom forward logic ... class CustomActorCritic ( ActorCritic ): def __init__ ( self , model_factory , obs_space : ObsSpace , action_space : ActionSpace , cfg : Config , ): super () . __init__ ( obs_space , action_space , cfg ) self . encoder = CustomEncoder ( cfg , obs_space ) self . core = CustomCore ( cfg , self . encoder . get_out_size ()) self . decoder = CustomDecoder ( cfg , self . core . get_out_size ()) self . critic_linear = nn . Linear ( self . decoder . get_out_size ()) self . action_parameterization = self . get_action_parameterization ( self . decoder . get_out_size () ) def forward ( self , normalized_obs_dict , rnn_states , values_only = False ): # forward logic ... def register_model_components (): # register custom components with the factory # you can register an entire Actor Critic model global_model_factory () . register_actor_critic_factory ( CustomActorCritic ) # or individual components global_model_factory () . register_encoder_factory ( CustomEncoder ) global_model_factory () . register_core_factory ( CustomCore ) global_model_factory () . register_decoder_factory ( CustomDecoder ) def main (): \"\"\"Script entry point.\"\"\" register_model_components () cfg = parse_args () status = run_rl ( cfg ) return status if __name__ == \"__main__\" : sys . exit ( main ()) Custom multi-agent environments \u00b6 Multi-agent environments are expected to return lists of observations/dones/rewards (one item for every agent). It is expected that a multi-agent env exposes a property or a member variable num_agents that the algorithm uses to allocate the right amount of memory during startup. Multi-agent environments require auto-reset. I.e. they reset a particular agent when the corresponding done flag is True and return the first observation of the next episode (because we have no use for the last observation of the previous episode, we do not act based on it). See multi_agent_wrapper.py for example. For simplicity Sample Factory actually treats all environments as multi-agent, i.e. single-agent environments are automatically treated as multi-agent environments with one agent. Sample Factory uses this function to check if the environment is multi-agent. Make sure your environment provides the num_agents member: def is_multiagent_env ( env ): is_multiagent = hasattr ( env , 'num_agents' ) and env . num_agents > 1 if hasattr ( env , 'is_multiagent' ): is_multiagent = env . is_multiagent return is_multiagent","title":"Customizing Sample Factory"},{"location":"get-started/customizing/#customizing-sample-factory","text":"","title":"Customizing Sample Factory"},{"location":"get-started/customizing/#custom-environments","text":"Training agents in your own environment with sample-factory is straightforward, but if you get stuck feel free to raise an issue on our GitHub Page . We recommend look at our example environment integrations such as Atari or ViZDoom before using your own environment.","title":"Custom environments"},{"location":"get-started/customizing/#custom-env-template","text":"We provide the following template, which you can modify to intergrate your environment. We assume your environment conforms to a gym 0.26 API (5-tuple). First make a file called train_custom_env.py and copy the following template. from sample_factory.cfg.arguments import parse_full_cfg , parse_sf_args from sample_factory.envs.env_utils import register_env from sample_factory.train import run_rl def make_custom_env ( env_name , cfg , env_config ): # function that build your custom env # cfg and env_config can be used to further customize the env # env_config.env_id can be used to seed each env, for example # env = create_your_custom_env() return env def register_custom_env_envs (): # register the env in sample-factory's global env registry register_env ( \"custom_env_name\" , make_custom_env ) def add_custom_env_args ( _env , p : argparse . ArgumentParser , evaluation = False ): # You can extend the command line arguments here p . add_argument ( \"--custom_argument\" , default = \"value\" , type = str , help = \"\" ) def custom_env_override_defaults ( _env , parser ): # Modify the default arguments when using this env # these can still be changed from the command line parser . set_defaults ( encoder_conv_architecture = \"convnet_atari\" , obs_scale = 255.0 , gamma = 0.99 , learning_rate = 0.00025 , lr_schedule = \"linear_decay\" , adam_eps = 1e-5 , ) def parse_args ( argv = None , evaluation = False ): # parse the command line arguments to build parser , partial_cfg = parse_sf_args ( argv = argv , evaluation = evaluation ) add_custom_env_args ( partial_cfg . env , parser , evaluation = evaluation ) custom_env_override_defaults ( partial_cfg . env , parser ) final_cfg = parse_full_cfg ( parser , argv ) return final_cfg def main (): \"\"\"Script entry point.\"\"\" register_custom_env_envs () cfg = parse_args () status = run_rl ( cfg ) return status if __name__ == \"__main__\" : sys . exit ( main ()) Training can now be started with python train_custom_env --env=custom_env_name","title":"Custom Env template"},{"location":"get-started/customizing/#custom-models","text":"Adding custom models in sample factory is simple, but if you get stuck feel free to raise an issue on our GitHub Page .","title":"Custom models"},{"location":"get-started/customizing/#actor-critic-models-in-sample-factory","text":"Actor Critic models in Sample Factory are composed of three components: Encoders - Process input observations (images, vectors) and map them to a vector. Cores - Intergrate vectors from one or more encoders, can optionally include an LSTM in a memory-based agent. Decoders - Apply addition linear layers to the output of the model core. You can register custom versions of each component, or you can register an entire Actor Critic model.","title":"Actor Critic models in sample factory"},{"location":"get-started/customizing/#custom-model-template","text":"from sample_factory.model.encoder import Encoder from sample_factory.model.decoder import Decoder from sample_factory.model.core import ModelCore from sample_factory.model.actor_critic import ActorCritic from sample_factory.algo.utils.context import global_model_factory class CustomEncoder ( Encoder ): def __init__ ( self , cfg : Config , obs_space : ObsSpace ): super () . __init__ ( cfg ) # build custom encoder architecture ... def forward ( self , obs_dict ): # custom forward logic ... class CustomCore ( ModelCore ): def __init__ ( self , cfg : Config , input_size : int ): super () . __init__ ( cfg ) # build custom core architecture ... def forward ( self , head_output , rnn_states ): # custom forward logic ... class CustomDecoder ( Decoder ): def __init__ ( self , cfg : Config , decoder_input_size : int ): super () . __init__ ( cfg ) # build custom decoder architecture ... def forward ( self , core_output ): # custom forward logic ... class CustomActorCritic ( ActorCritic ): def __init__ ( self , model_factory , obs_space : ObsSpace , action_space : ActionSpace , cfg : Config , ): super () . __init__ ( obs_space , action_space , cfg ) self . encoder = CustomEncoder ( cfg , obs_space ) self . core = CustomCore ( cfg , self . encoder . get_out_size ()) self . decoder = CustomDecoder ( cfg , self . core . get_out_size ()) self . critic_linear = nn . Linear ( self . decoder . get_out_size ()) self . action_parameterization = self . get_action_parameterization ( self . decoder . get_out_size () ) def forward ( self , normalized_obs_dict , rnn_states , values_only = False ): # forward logic ... def register_model_components (): # register custom components with the factory # you can register an entire Actor Critic model global_model_factory () . register_actor_critic_factory ( CustomActorCritic ) # or individual components global_model_factory () . register_encoder_factory ( CustomEncoder ) global_model_factory () . register_core_factory ( CustomCore ) global_model_factory () . register_decoder_factory ( CustomDecoder ) def main (): \"\"\"Script entry point.\"\"\" register_model_components () cfg = parse_args () status = run_rl ( cfg ) return status if __name__ == \"__main__\" : sys . exit ( main ())","title":"Custom model template"},{"location":"get-started/customizing/#custom-multi-agent-environments","text":"Multi-agent environments are expected to return lists of observations/dones/rewards (one item for every agent). It is expected that a multi-agent env exposes a property or a member variable num_agents that the algorithm uses to allocate the right amount of memory during startup. Multi-agent environments require auto-reset. I.e. they reset a particular agent when the corresponding done flag is True and return the first observation of the next episode (because we have no use for the last observation of the previous episode, we do not act based on it). See multi_agent_wrapper.py for example. For simplicity Sample Factory actually treats all environments as multi-agent, i.e. single-agent environments are automatically treated as multi-agent environments with one agent. Sample Factory uses this function to check if the environment is multi-agent. Make sure your environment provides the num_agents member: def is_multiagent_env ( env ): is_multiagent = hasattr ( env , 'num_agents' ) and env . num_agents > 1 if hasattr ( env , 'is_multiagent' ): is_multiagent = env . is_multiagent return is_multiagent","title":"Custom multi-agent environments"},{"location":"get-started/experiment-launcher/","text":"SampleFactory Launcher API reference \u00b6 Sample Factory provides a simple interface that allows users to run experiments with multiple seeds (or hyperparameter searches) with optimal distribution of work across GPUs. The configuration of such experiments is done through Python scripts. See README for more general information. Command-line interface \u00b6 CLI Examples: \u00b6 Parallelize with local multiprocessing: $ python -m sample_factory.launcher.run --run=paper_doom_battle2_appo --backend=processes --max_parallel=4 --pause_between=10 --experiments_per_gpu=1 --num_gpus=4 Parallelize with Slurm: $ python -m sample_factory.launcher.run --run=megaverse_rl.runs.single_agent --backend=slurm --slurm_workdir=./megaverse_single_agent --experiment_suffix=slurm --pause_between=1 --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=12 --slurm_sbatch_template=./megaverse_rl/slurm/sbatch_template.sh --slurm_print_only=False Parallelize with NGC (https://ngc.nvidia.com/): $ python -m sample_factory.launcher.run --run=rlgpu.run_scripts.dexterous_manipulation --backend=ngc --ngc_job_template=run_scripts/ngc_job_16g_1gpu.template --ngc_print_only=False --train_dir=/workspace/train_dir All command-line options: \u00b6 usage: run.py [-h] [--train_dir TRAIN_DIR] [--run RUN] [--backend {processes,slurm,ngc}] [--pause_between PAUSE_BETWEEN] [--num_gpus NUM_GPUS] [--experiments_per_gpu EXPERIMENTS_PER_GPU] [--max_parallel MAX_PARALLEL] [--experiment_suffix EXPERIMENT_SUFFIX] # Slurm-related: [--slurm_gpus_per_job SLURM_GPUS_PER_JOB] [--slurm_cpus_per_gpu SLURM_CPUS_PER_GPU] [--slurm_print_only SLURM_PRINT_ONLY] [--slurm_workdir SLURM_WORKDIR] [--slurm_partition SLURM_PARTITION] [--slurm_sbatch_template SLURM_SBATCH_TEMPLATE] # NGC-related [--ngc_job_template NGC_JOB_TEMPLATE] [--ngc_print_only NGC_PRINT_ONLY] Arguments: -h, --help show this help message and exit --train_dir TRAIN_DIR Directory for sub-experiments --run RUN Name of the python module that describes the run, e.g. sf_examples.vizdoom.experiments.doom_basic --backend {processes,slurm,ngc} --pause_between PAUSE_BETWEEN Pause in seconds between processes --num_gpus NUM_GPUS How many GPUs to use (only for local multiprocessing) --experiments_per_gpu EXPERIMENTS_PER_GPU How many experiments can we squeeze on a single GPU (-1 for not altering CUDA_VISIBLE_DEVICES at all) --max_parallel MAX_PARALLEL Maximum simultaneous experiments (only for local multiprocessing) --experiment_suffix EXPERIMENT_SUFFIX Append this to the name of the experiment dir Slurm-related: --slurm_gpus_per_job SLURM_GPUS_PER_JOB GPUs in a single SLURM process --slurm_cpus_per_gpu SLURM_CPUS_PER_GPU Max allowed number of CPU cores per allocated GPU --slurm_print_only SLURM_PRINT_ONLY Just print commands to the console without executing --slurm_workdir SLURM_WORKDIR Optional workdir. Used by slurm launcher to store logfiles etc. --slurm_partition SLURM_PARTITION Adds slurm partition, i.e. for \"gpu\" it will add \"-p gpu\" to sbatch command line --slurm_sbatch_template SLURM_SBATCH_TEMPLATE Commands to run before the actual experiment (i.e. activate conda env, etc.) Example: https://github.com/alex-petrenko/megaverse/blob/master/megaverse_rl/slurm/sbatch_template.sh (typically a shell script) NGC-related: --ngc_job_template NGC_JOB_TEMPLATE NGC command line template, specifying instance type, docker container, etc. --ngc_print_only NGC_PRINT_ONLY Just print commands to the console without executing Launcher script API \u00b6 A typical launcher script: from sample_factory.launcher.run_description import RunDescription, Experiment, ParamGrid _params = ParamGrid([ ('seed', [0, 1111, 2222, 3333, 4444, 5555, 6666, 7777, 8888, 9999]), ('env', ['doom_my_way_home', 'doom_deadly_corridor', 'doom_defend_the_center', 'doom_defend_the_line', 'doom_health_gathering', 'doom_health_gathering_supreme']), ]) _experiments = [ Experiment( 'basic_envs_fs4', 'python -m sample_factory.algorithms.appo.train_appo --train_for_env_steps=500000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=36 --num_envs_per_worker=8 --num_policies=1 --ppo_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --wide_aspect_ratio=False', _params.generate_params(randomize=False), ), ] RUN_DESCRIPTION = RunDescription('doom_basic_envs_appo', experiments=_experiments) Launcher script should expose a RunDescription object named RUN_DESCRIPTION that contains a list of experiments to run and some auxiliary parameters. RunDescription parameter reference: def __init__( self, run_name, experiments, experiment_dirs_sf_format=True, experiment_arg_name='--experiment', experiment_dir_arg_name='--train_dir', customize_experiment_name=True, param_prefix='--', ): \"\"\" :param run_name: overall name of the experiment and the name of the root folder :param experiments: a list of Experiment objects to run :param experiment_dirs_sf_format: adds an additional --experiments_root parameter, used only by SampleFactory. set to False for other applications. :param experiment_arg_name: CLI argument of the underlying experiment that determines it's unique name to be generated by the launcher. Default: --experiment :param experiment_dir_arg_name: CLI argument for the root train dir of your experiment. Default: --train_dir :param customize_experiment_name: whether to add a hyperparameter combination to the experiment name :param param_prefix: most experiments will use \"--\" prefix for each parameter, but some apps don't have this prefix, i.e. with Hydra you should set it to empty string. \"\"\"","title":"Experiment launcher"},{"location":"get-started/experiment-launcher/#samplefactory-launcher-api-reference","text":"Sample Factory provides a simple interface that allows users to run experiments with multiple seeds (or hyperparameter searches) with optimal distribution of work across GPUs. The configuration of such experiments is done through Python scripts. See README for more general information.","title":"SampleFactory Launcher API reference"},{"location":"get-started/experiment-launcher/#command-line-interface","text":"","title":"Command-line interface"},{"location":"get-started/experiment-launcher/#cli-examples","text":"Parallelize with local multiprocessing: $ python -m sample_factory.launcher.run --run=paper_doom_battle2_appo --backend=processes --max_parallel=4 --pause_between=10 --experiments_per_gpu=1 --num_gpus=4 Parallelize with Slurm: $ python -m sample_factory.launcher.run --run=megaverse_rl.runs.single_agent --backend=slurm --slurm_workdir=./megaverse_single_agent --experiment_suffix=slurm --pause_between=1 --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=12 --slurm_sbatch_template=./megaverse_rl/slurm/sbatch_template.sh --slurm_print_only=False Parallelize with NGC (https://ngc.nvidia.com/): $ python -m sample_factory.launcher.run --run=rlgpu.run_scripts.dexterous_manipulation --backend=ngc --ngc_job_template=run_scripts/ngc_job_16g_1gpu.template --ngc_print_only=False --train_dir=/workspace/train_dir","title":"CLI Examples:"},{"location":"get-started/experiment-launcher/#all-command-line-options","text":"usage: run.py [-h] [--train_dir TRAIN_DIR] [--run RUN] [--backend {processes,slurm,ngc}] [--pause_between PAUSE_BETWEEN] [--num_gpus NUM_GPUS] [--experiments_per_gpu EXPERIMENTS_PER_GPU] [--max_parallel MAX_PARALLEL] [--experiment_suffix EXPERIMENT_SUFFIX] # Slurm-related: [--slurm_gpus_per_job SLURM_GPUS_PER_JOB] [--slurm_cpus_per_gpu SLURM_CPUS_PER_GPU] [--slurm_print_only SLURM_PRINT_ONLY] [--slurm_workdir SLURM_WORKDIR] [--slurm_partition SLURM_PARTITION] [--slurm_sbatch_template SLURM_SBATCH_TEMPLATE] # NGC-related [--ngc_job_template NGC_JOB_TEMPLATE] [--ngc_print_only NGC_PRINT_ONLY] Arguments: -h, --help show this help message and exit --train_dir TRAIN_DIR Directory for sub-experiments --run RUN Name of the python module that describes the run, e.g. sf_examples.vizdoom.experiments.doom_basic --backend {processes,slurm,ngc} --pause_between PAUSE_BETWEEN Pause in seconds between processes --num_gpus NUM_GPUS How many GPUs to use (only for local multiprocessing) --experiments_per_gpu EXPERIMENTS_PER_GPU How many experiments can we squeeze on a single GPU (-1 for not altering CUDA_VISIBLE_DEVICES at all) --max_parallel MAX_PARALLEL Maximum simultaneous experiments (only for local multiprocessing) --experiment_suffix EXPERIMENT_SUFFIX Append this to the name of the experiment dir Slurm-related: --slurm_gpus_per_job SLURM_GPUS_PER_JOB GPUs in a single SLURM process --slurm_cpus_per_gpu SLURM_CPUS_PER_GPU Max allowed number of CPU cores per allocated GPU --slurm_print_only SLURM_PRINT_ONLY Just print commands to the console without executing --slurm_workdir SLURM_WORKDIR Optional workdir. Used by slurm launcher to store logfiles etc. --slurm_partition SLURM_PARTITION Adds slurm partition, i.e. for \"gpu\" it will add \"-p gpu\" to sbatch command line --slurm_sbatch_template SLURM_SBATCH_TEMPLATE Commands to run before the actual experiment (i.e. activate conda env, etc.) Example: https://github.com/alex-petrenko/megaverse/blob/master/megaverse_rl/slurm/sbatch_template.sh (typically a shell script) NGC-related: --ngc_job_template NGC_JOB_TEMPLATE NGC command line template, specifying instance type, docker container, etc. --ngc_print_only NGC_PRINT_ONLY Just print commands to the console without executing","title":"All command-line options:"},{"location":"get-started/experiment-launcher/#launcher-script-api","text":"A typical launcher script: from sample_factory.launcher.run_description import RunDescription, Experiment, ParamGrid _params = ParamGrid([ ('seed', [0, 1111, 2222, 3333, 4444, 5555, 6666, 7777, 8888, 9999]), ('env', ['doom_my_way_home', 'doom_deadly_corridor', 'doom_defend_the_center', 'doom_defend_the_line', 'doom_health_gathering', 'doom_health_gathering_supreme']), ]) _experiments = [ Experiment( 'basic_envs_fs4', 'python -m sample_factory.algorithms.appo.train_appo --train_for_env_steps=500000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=36 --num_envs_per_worker=8 --num_policies=1 --ppo_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --wide_aspect_ratio=False', _params.generate_params(randomize=False), ), ] RUN_DESCRIPTION = RunDescription('doom_basic_envs_appo', experiments=_experiments) Launcher script should expose a RunDescription object named RUN_DESCRIPTION that contains a list of experiments to run and some auxiliary parameters. RunDescription parameter reference: def __init__( self, run_name, experiments, experiment_dirs_sf_format=True, experiment_arg_name='--experiment', experiment_dir_arg_name='--train_dir', customize_experiment_name=True, param_prefix='--', ): \"\"\" :param run_name: overall name of the experiment and the name of the root folder :param experiments: a list of Experiment objects to run :param experiment_dirs_sf_format: adds an additional --experiments_root parameter, used only by SampleFactory. set to False for other applications. :param experiment_arg_name: CLI argument of the underlying experiment that determines it's unique name to be generated by the launcher. Default: --experiment :param experiment_dir_arg_name: CLI argument for the root train dir of your experiment. Default: --train_dir :param customize_experiment_name: whether to add a hyperparameter combination to the experiment name :param param_prefix: most experiments will use \"--\" prefix for each parameter, but some apps don't have this prefix, i.e. with Hydra you should set it to empty string. \"\"\"","title":"Launcher script API"},{"location":"get-started/huggingface/","text":"Hugging Face \ud83e\udd17 Hub Integration \u00b6 Sample Factory has integrations with \ud83e\udd17 Hugging Face Hub to push models with evaluation results and training metrics to the hub. Setting Up \u00b6 The Hugging Face Hub requires git lfs to download model files. sudo apt install git-lfs git lfs install To upload files to the Hugging Face Hub, you need to sign up and log in to your Hugging Face account with: huggingface-cli login As part of the huggingface-cli login , you should generate a token with write access at https://huggingface.co/settings/tokens Downloading Models \u00b6 Using the load_from_hub Scipt \u00b6 To download a model from the Hugging Face Hub to use with Sample-Factory, use the load_from_hub script: python -m sample_factory.huggingface.load_from_hub -r <HuggingFace_repo_id> -d <train_dir_path> The command line arguments are: -r : The repo ID for the HF repository to download. The repo ID should be in the format <username>/<repo_name> -d : An optional argument to specify the directory to save the experiment to. Defaults to ./train_dir which will save the repo to ./train_dir/<repo_name> Download Model Repository Directly \u00b6 Hugging Face repositories can be downloaded directly using git clone : git clone <URL of Hugging Face Repo> Using Downloaded Models with Sample-Factory \u00b6 After downloading the model, you can run the models in the repo with the enjoy script corresponding to your environment. For example, if you are downloading a mujoco-ant model, it can be run with: python -m sf_examples.mujoco.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<repo_name> --train_dir=./train_dir Note, you may have to specify the --train_dir if your local train_dir has a different path than the one in the cfg.json Uploading Models \u00b6 Using enjoy.py \u00b6 You can upload your models to the Hub using your environment's enjoy script with the --push_to_hub flag. Uploading using enjoy can also generate evaluation metrics and a replay video. The evaluation metrics are generated by running your model on the specified environment for a number of episodes and reporting the mean and std reward of those runs. Other relevant command line arguments are: --hf_repository : The repository to push to. Must be of the form <username>/<repo_name> . The model will be saved to https://huggingface.co/<username>/<repo_name> --max_num_episodes : Number of episodes to evaluate on before uploading. Used to generate evaluation metrics. It is recommended to use multiple episodes to generate an accurate mean and std. --max_num_frames : Number of frames to evaluate on before uploading. An alternative to max_num_episodes --no_render : A flag that disables rendering and showing the environment steps. It is recommended to set this flag to speed up the evaluation process. You can also save a video of the model during evaluation to upload to the hub with the --save_video flag --video_frames : The number of frames to be rendered in the video. Defaults to -1 which renders an entire episode --video_name : The name of the video to save as. If None , will save to replay.mp4 in your experiment directory For example: python -m sf_examples.mujoco.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<repo_name> --train_dir=./train_dir --max_num_episodes=10 --push_to_hub --hf_repository=<username>/<hf_repo_name> --save_video --no_render Using the push_to_hub Script \u00b6 If you want to upload without generating evaluation metrics or a replay video, you can use the push_to_hub script: python -m sample_factory.huggingface.push_to_hub -r <hf_username>/<hf_repo_name> -d <experiment_dir_path> The command line arguments are: -r : The repo_id to save on HF Hub. This is the same as hf_repository in the enjoy script and must be in the form <hf_username>/<hf_repo_name> -d : The full path to your experiment directory to upload","title":"Hugging Face \ud83e\udd17 Hub Integration"},{"location":"get-started/huggingface/#hugging-face-hub-integration","text":"Sample Factory has integrations with \ud83e\udd17 Hugging Face Hub to push models with evaluation results and training metrics to the hub.","title":"Hugging Face \ud83e\udd17 Hub Integration"},{"location":"get-started/huggingface/#setting-up","text":"The Hugging Face Hub requires git lfs to download model files. sudo apt install git-lfs git lfs install To upload files to the Hugging Face Hub, you need to sign up and log in to your Hugging Face account with: huggingface-cli login As part of the huggingface-cli login , you should generate a token with write access at https://huggingface.co/settings/tokens","title":"Setting Up"},{"location":"get-started/huggingface/#downloading-models","text":"","title":"Downloading Models"},{"location":"get-started/huggingface/#using-the-load_from_hub-scipt","text":"To download a model from the Hugging Face Hub to use with Sample-Factory, use the load_from_hub script: python -m sample_factory.huggingface.load_from_hub -r <HuggingFace_repo_id> -d <train_dir_path> The command line arguments are: -r : The repo ID for the HF repository to download. The repo ID should be in the format <username>/<repo_name> -d : An optional argument to specify the directory to save the experiment to. Defaults to ./train_dir which will save the repo to ./train_dir/<repo_name>","title":"Using the load_from_hub Scipt"},{"location":"get-started/huggingface/#download-model-repository-directly","text":"Hugging Face repositories can be downloaded directly using git clone : git clone <URL of Hugging Face Repo>","title":"Download Model Repository Directly"},{"location":"get-started/huggingface/#using-downloaded-models-with-sample-factory","text":"After downloading the model, you can run the models in the repo with the enjoy script corresponding to your environment. For example, if you are downloading a mujoco-ant model, it can be run with: python -m sf_examples.mujoco.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<repo_name> --train_dir=./train_dir Note, you may have to specify the --train_dir if your local train_dir has a different path than the one in the cfg.json","title":"Using Downloaded Models with Sample-Factory"},{"location":"get-started/huggingface/#uploading-models","text":"","title":"Uploading Models"},{"location":"get-started/huggingface/#using-enjoypy","text":"You can upload your models to the Hub using your environment's enjoy script with the --push_to_hub flag. Uploading using enjoy can also generate evaluation metrics and a replay video. The evaluation metrics are generated by running your model on the specified environment for a number of episodes and reporting the mean and std reward of those runs. Other relevant command line arguments are: --hf_repository : The repository to push to. Must be of the form <username>/<repo_name> . The model will be saved to https://huggingface.co/<username>/<repo_name> --max_num_episodes : Number of episodes to evaluate on before uploading. Used to generate evaluation metrics. It is recommended to use multiple episodes to generate an accurate mean and std. --max_num_frames : Number of frames to evaluate on before uploading. An alternative to max_num_episodes --no_render : A flag that disables rendering and showing the environment steps. It is recommended to set this flag to speed up the evaluation process. You can also save a video of the model during evaluation to upload to the hub with the --save_video flag --video_frames : The number of frames to be rendered in the video. Defaults to -1 which renders an entire episode --video_name : The name of the video to save as. If None , will save to replay.mp4 in your experiment directory For example: python -m sf_examples.mujoco.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<repo_name> --train_dir=./train_dir --max_num_episodes=10 --push_to_hub --hf_repository=<username>/<hf_repo_name> --save_video --no_render","title":"Using enjoy.py"},{"location":"get-started/huggingface/#using-the-push_to_hub-script","text":"If you want to upload without generating evaluation metrics or a replay video, you can use the push_to_hub script: python -m sample_factory.huggingface.push_to_hub -r <hf_username>/<hf_repo_name> -d <experiment_dir_path> The command line arguments are: -r : The repo_id to save on HF Hub. This is the same as hf_repository in the enjoy script and must be in the form <hf_username>/<hf_repo_name> -d : The full path to your experiment directory to upload","title":"Using the push_to_hub Script"},{"location":"get-started/installation/","text":"Installation \u00b6 Using pip \u00b6 Just install from PyPI: pip install sample-factory SF is known to work on Linux and macOS. There is no Windows support at this time. Environment support \u00b6 Sample Factory has a runtime environment registry for families of environments . A family of environments is defined by a name prefix (i.e. atari_ or doom_ ) and a function that creates an instance of the environment given its full name, including the prefix (i.e. atari_breakout ). Registering families of environments allows the user to add and override configuration parameters (such as resolution, frameskip, default model type, etc.) for the whole family of environments, i.e. all VizDoom envs can share their basic configuration parameters that don't need to be specified for each experiment. Custom user-defined environment families and models can be added to the registry, see this example: sample_factory_examples/train_custom_env_custom_model.py Script sample_factory_examples/train_gym_env.py demonstrates how Sample Factory can be used with an environment defined in OpenAI Gym. Sample Factory comes with comprehensive support Mujoco, Atari, VizDoom, DMLab, Megaverse and Envpool: Mujoco Atari ViZDoom DeepMind Lab Megaverse Envpool Sample Factory allows users to easily add custom environments and models, refer to Customizing Sample Factory for more information.","title":"Installation"},{"location":"get-started/installation/#installation","text":"","title":"Installation"},{"location":"get-started/installation/#using-pip","text":"Just install from PyPI: pip install sample-factory SF is known to work on Linux and macOS. There is no Windows support at this time.","title":"Using pip"},{"location":"get-started/installation/#environment-support","text":"Sample Factory has a runtime environment registry for families of environments . A family of environments is defined by a name prefix (i.e. atari_ or doom_ ) and a function that creates an instance of the environment given its full name, including the prefix (i.e. atari_breakout ). Registering families of environments allows the user to add and override configuration parameters (such as resolution, frameskip, default model type, etc.) for the whole family of environments, i.e. all VizDoom envs can share their basic configuration parameters that don't need to be specified for each experiment. Custom user-defined environment families and models can be added to the registry, see this example: sample_factory_examples/train_custom_env_custom_model.py Script sample_factory_examples/train_gym_env.py demonstrates how Sample Factory can be used with an environment defined in OpenAI Gym. Sample Factory comes with comprehensive support Mujoco, Atari, VizDoom, DMLab, Megaverse and Envpool: Mujoco Atari ViZDoom DeepMind Lab Megaverse Envpool Sample Factory allows users to easily add custom environments and models, refer to Customizing Sample Factory for more information.","title":"Environment support"},{"location":"get-started/running-experiments/","text":"Running Experiments \u00b6 Here we provide command lines that can be used to reproduce the experiments from the paper, which also serve as an example on how to configure large-scale RL experiments. DMLab \u00b6 DMLab-30 run on a 36-core server with 4 GPUs: python -m sample_factory.algorithms.appo.train_appo --env=dmlab_30 --train_for_seconds=3600000 --algo=APPO --gamma=0.99 --use_rnn=True --num_workers=90 --num_envs_per_worker=12 --ppo_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --benchmark=False --ppo_epochs=1 --max_grad_norm=0.0 --dmlab_renderer=software --decorrelate_experience_max_seconds=120 --reset_timeout_seconds=300 --encoder_custom=dmlab_instructions --encoder_type=resnet --encoder_subtype=resnet_impala --encoder_extra_fc_layers=1 --hidden_size=256 --nonlinearity=relu --rnn_type=lstm --dmlab_extended_action_set=True --num_policies=4 --pbt_replace_reward_gap=0.05 --pbt_replace_reward_gap_absolute=5.0 --pbt_period_env_steps=10000000 --pbt_start_mutation=100000000 --with_pbt=True --experiment=dmlab_30_resnet_4pbt_w90_v12 --dmlab_one_task_per_worker=True --set_workers_cpu_affinity=True --max_policy_lag=35 --pbt_target_objective=dmlab_target_objective --dmlab30_dataset=~/datasets/brady_konkle_oliva2008 --dmlab_use_level_cache=True --dmlab_level_cache_path=/home/user/dmlab_cache DMLab level cache \u00b6 Note --dmlab_level_cache_path parameter. This location will be used for level layout cache. Subsequent DMLab experiments on envs that require level generation will become faster since environment files from previous runs can be reused. Generating environment levels for the first time can be really slow, especially for the full multi-task benchmark like DMLab-30. On 36-core server generating enough environments for a 10B training session can take up to a week. We provide a dataset of pre-generated levels to make training on DMLab-30 easier. Download here . Monitoring training sessions \u00b6 Sample Factory uses Tensorboard summaries. Run Tensorboard to monitor your experiment: tensorboard --logdir=train_dir --port=6006 Additionally, we provide a helper script that has nice command line interface to monitor the experiment folders using wildcard masks: python -m sample_factory.tb '*custom_experiment*' '*another*custom*experiment_name' WandB support \u00b6 Sample Factory also supports experiment monitoring with Weights and Biases. In order to setup WandB locally run wandb login in the terminal ( https://docs.wandb.ai/quickstart#1.-set-up-wandb ) Example command line to run an experiment with WandB monitoring: python -m sample_factory.algorithms.appo.train_appo --env=doom_basic --algo=APPO --train_for_env_steps=30000000 --num_workers=20 --num_envs_per_worker=20 --experiment=doom_basic --with_wandb=True --wandb_user=<your_wandb_user> --wandb_tags test benchmark doom appo A total list of WandB settings: --with_wandb: Enables Weights and Biases integration (default: False) --wandb_user: WandB username (entity). Must be specified from command line! Also see https://docs.wandb.ai/quickstart#1.-set-up-wandb (default: None) --wandb_project: WandB \"Project\" (default: sample_factory) --wandb_group: WandB \"Group\" (to group your experiments). By default this is the name of the env. (default: None) --wandb_job_type: WandB job type (default: SF) --wandb_tags: [WANDB_TAGS [WANDB_TAGS ...]] Tags can help with finding experiments in WandB web console (default: []) Once the experiment is started the link to the monitored session is going to be available in the logs (or by searching in Wandb Web console). Launcher interface \u00b6 Sample Factory provides a simple interface that allows users to run experiments with multiple seeds (or hyperparameter searches) with optimal distribution of work across GPUs. The configuration of such experiments is done through Python scripts. Here's an example launcher script that we used to train agents for 6 basic VizDoom environments with 10 seeds each: from sample_factory.launcher.run_description import RunDescription, Experiment, ParamGrid _params = ParamGrid([ ('seed', [0, 1111, 2222, 3333, 4444, 5555, 6666, 7777, 8888, 9999]), ('env', ['doom_my_way_home', 'doom_deadly_corridor', 'doom_defend_the_center', 'doom_defend_the_line', 'doom_health_gathering', 'doom_health_gathering_supreme']), ]) _experiments = [ Experiment( 'basic_envs_fs4', 'python -m sample_factory.algorithms.appo.train_appo --train_for_env_steps=500000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=36 --num_envs_per_worker=8 --num_policies=1 --ppo_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --wide_aspect_ratio=False', _params.generate_params(randomize=False), ), ] RUN_DESCRIPTION = RunDescription('doom_basic_envs_appo', experiments=_experiments) Runner script should be importable (i.e. be in your project or in PYTHONPATH), and should define a single variable RUN_DESCRIPTION , which contains a list of experiments (each experiment can be a hyperparameter search), as well as some auxiliary parameters. When such a script is saved i.e. at myproject/train_10_seeds.py in your project using Sample Factory, you can use this command to execute it: python -m sample_factory.launcher.run --run=myproject.train_10_seeds --backend=processes --max_parallel=12 --pause_between=10 --experiments_per_gpu=3 --num_gpus=4 This will cycle through the requested configurations, training 12 experiments at the same time, 3 per GPU on 4 GPUs using local OS-level parallelism. Runner supports other backends for parallel execution: --backend=slurm and --backend=ngc for Slurm and NGC support respectively. Individual experiments will be stored in train_dir/run_name so the whole experiment can be easily monitored with a single Tensorboard command. Find more information on runner API in runner/README.md . Dummy sampler \u00b6 This tool can be useful if you want to estimate the upper bound on performance of any reinforcement learning algorithm, i.e. how fast the environment can be sampled by a dumb random policy. This achieves 90000+ FPS on a 10-core workstation: python -m sample_factory.run_algorithm --algo=DUMMY_SAMPLER --env=doom_benchmark --num_workers=20 --num_envs_per_worker=1 --experiment=dummy_sampler --sample_env_frames=5000000 Tests \u00b6 To run unit tests execute ./all_tests.sh from the root of the repo. Consider installing VizDoom for a more comprehensive set of tests. Trained policies \u00b6 See a separate trained_policies/README.md . Caveats \u00b6 Multiplayer VizDoom environments can freeze your console sometimes, simple reset takes care of this Sometimes VizDoom instances don't clear their internal shared memory buffers used to communicate between Python and a Doom executable. The file descriptors for these buffers tend to pile up. rm /dev/shm/ViZDoom* will take care of this issue. It's best to use the standard --fps=35 to visualize VizDoom results. --fps=0 enables Async execution mode for the Doom environments, although the results are not always reproducible between sync and async modes. Multiplayer VizDoom environments are significantly slower than single-player envs because actual network communication between the environment instances is required which results in a lot of syscalls. For prototyping and testing consider single-player environments with bots instead. Vectors of environments on rollout (actor) workers are instantiated on the same CPU thread. This can create problems for certain types of environment that require global per-thread or per-process context (e.g. OpenGL context). The solution should be an environment wrapper that starts the environment in a separate thread (or process if that's required) and communicates. doom_multiagent_wrapper.py is an example, although not optimal.","title":"Running Experiments"},{"location":"get-started/running-experiments/#running-experiments","text":"Here we provide command lines that can be used to reproduce the experiments from the paper, which also serve as an example on how to configure large-scale RL experiments.","title":"Running Experiments"},{"location":"get-started/running-experiments/#dmlab","text":"DMLab-30 run on a 36-core server with 4 GPUs: python -m sample_factory.algorithms.appo.train_appo --env=dmlab_30 --train_for_seconds=3600000 --algo=APPO --gamma=0.99 --use_rnn=True --num_workers=90 --num_envs_per_worker=12 --ppo_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --benchmark=False --ppo_epochs=1 --max_grad_norm=0.0 --dmlab_renderer=software --decorrelate_experience_max_seconds=120 --reset_timeout_seconds=300 --encoder_custom=dmlab_instructions --encoder_type=resnet --encoder_subtype=resnet_impala --encoder_extra_fc_layers=1 --hidden_size=256 --nonlinearity=relu --rnn_type=lstm --dmlab_extended_action_set=True --num_policies=4 --pbt_replace_reward_gap=0.05 --pbt_replace_reward_gap_absolute=5.0 --pbt_period_env_steps=10000000 --pbt_start_mutation=100000000 --with_pbt=True --experiment=dmlab_30_resnet_4pbt_w90_v12 --dmlab_one_task_per_worker=True --set_workers_cpu_affinity=True --max_policy_lag=35 --pbt_target_objective=dmlab_target_objective --dmlab30_dataset=~/datasets/brady_konkle_oliva2008 --dmlab_use_level_cache=True --dmlab_level_cache_path=/home/user/dmlab_cache","title":"DMLab"},{"location":"get-started/running-experiments/#dmlab-level-cache","text":"Note --dmlab_level_cache_path parameter. This location will be used for level layout cache. Subsequent DMLab experiments on envs that require level generation will become faster since environment files from previous runs can be reused. Generating environment levels for the first time can be really slow, especially for the full multi-task benchmark like DMLab-30. On 36-core server generating enough environments for a 10B training session can take up to a week. We provide a dataset of pre-generated levels to make training on DMLab-30 easier. Download here .","title":"DMLab level cache"},{"location":"get-started/running-experiments/#monitoring-training-sessions","text":"Sample Factory uses Tensorboard summaries. Run Tensorboard to monitor your experiment: tensorboard --logdir=train_dir --port=6006 Additionally, we provide a helper script that has nice command line interface to monitor the experiment folders using wildcard masks: python -m sample_factory.tb '*custom_experiment*' '*another*custom*experiment_name'","title":"Monitoring training sessions"},{"location":"get-started/running-experiments/#wandb-support","text":"Sample Factory also supports experiment monitoring with Weights and Biases. In order to setup WandB locally run wandb login in the terminal ( https://docs.wandb.ai/quickstart#1.-set-up-wandb ) Example command line to run an experiment with WandB monitoring: python -m sample_factory.algorithms.appo.train_appo --env=doom_basic --algo=APPO --train_for_env_steps=30000000 --num_workers=20 --num_envs_per_worker=20 --experiment=doom_basic --with_wandb=True --wandb_user=<your_wandb_user> --wandb_tags test benchmark doom appo A total list of WandB settings: --with_wandb: Enables Weights and Biases integration (default: False) --wandb_user: WandB username (entity). Must be specified from command line! Also see https://docs.wandb.ai/quickstart#1.-set-up-wandb (default: None) --wandb_project: WandB \"Project\" (default: sample_factory) --wandb_group: WandB \"Group\" (to group your experiments). By default this is the name of the env. (default: None) --wandb_job_type: WandB job type (default: SF) --wandb_tags: [WANDB_TAGS [WANDB_TAGS ...]] Tags can help with finding experiments in WandB web console (default: []) Once the experiment is started the link to the monitored session is going to be available in the logs (or by searching in Wandb Web console).","title":"WandB support"},{"location":"get-started/running-experiments/#launcher-interface","text":"Sample Factory provides a simple interface that allows users to run experiments with multiple seeds (or hyperparameter searches) with optimal distribution of work across GPUs. The configuration of such experiments is done through Python scripts. Here's an example launcher script that we used to train agents for 6 basic VizDoom environments with 10 seeds each: from sample_factory.launcher.run_description import RunDescription, Experiment, ParamGrid _params = ParamGrid([ ('seed', [0, 1111, 2222, 3333, 4444, 5555, 6666, 7777, 8888, 9999]), ('env', ['doom_my_way_home', 'doom_deadly_corridor', 'doom_defend_the_center', 'doom_defend_the_line', 'doom_health_gathering', 'doom_health_gathering_supreme']), ]) _experiments = [ Experiment( 'basic_envs_fs4', 'python -m sample_factory.algorithms.appo.train_appo --train_for_env_steps=500000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=36 --num_envs_per_worker=8 --num_policies=1 --ppo_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --wide_aspect_ratio=False', _params.generate_params(randomize=False), ), ] RUN_DESCRIPTION = RunDescription('doom_basic_envs_appo', experiments=_experiments) Runner script should be importable (i.e. be in your project or in PYTHONPATH), and should define a single variable RUN_DESCRIPTION , which contains a list of experiments (each experiment can be a hyperparameter search), as well as some auxiliary parameters. When such a script is saved i.e. at myproject/train_10_seeds.py in your project using Sample Factory, you can use this command to execute it: python -m sample_factory.launcher.run --run=myproject.train_10_seeds --backend=processes --max_parallel=12 --pause_between=10 --experiments_per_gpu=3 --num_gpus=4 This will cycle through the requested configurations, training 12 experiments at the same time, 3 per GPU on 4 GPUs using local OS-level parallelism. Runner supports other backends for parallel execution: --backend=slurm and --backend=ngc for Slurm and NGC support respectively. Individual experiments will be stored in train_dir/run_name so the whole experiment can be easily monitored with a single Tensorboard command. Find more information on runner API in runner/README.md .","title":"Launcher interface"},{"location":"get-started/running-experiments/#dummy-sampler","text":"This tool can be useful if you want to estimate the upper bound on performance of any reinforcement learning algorithm, i.e. how fast the environment can be sampled by a dumb random policy. This achieves 90000+ FPS on a 10-core workstation: python -m sample_factory.run_algorithm --algo=DUMMY_SAMPLER --env=doom_benchmark --num_workers=20 --num_envs_per_worker=1 --experiment=dummy_sampler --sample_env_frames=5000000","title":"Dummy sampler"},{"location":"get-started/running-experiments/#tests","text":"To run unit tests execute ./all_tests.sh from the root of the repo. Consider installing VizDoom for a more comprehensive set of tests.","title":"Tests"},{"location":"get-started/running-experiments/#trained-policies","text":"See a separate trained_policies/README.md .","title":"Trained policies"},{"location":"get-started/running-experiments/#caveats","text":"Multiplayer VizDoom environments can freeze your console sometimes, simple reset takes care of this Sometimes VizDoom instances don't clear their internal shared memory buffers used to communicate between Python and a Doom executable. The file descriptors for these buffers tend to pile up. rm /dev/shm/ViZDoom* will take care of this issue. It's best to use the standard --fps=35 to visualize VizDoom results. --fps=0 enables Async execution mode for the Doom environments, although the results are not always reproducible between sync and async modes. Multiplayer VizDoom environments are significantly slower than single-player envs because actual network communication between the environment instances is required which results in a lot of syscalls. For prototyping and testing consider single-player environments with bots instead. Vectors of environments on rollout (actor) workers are instantiated on the same CPU thread. This can create problems for certain types of environment that require global per-thread or per-process context (e.g. OpenGL context). The solution should be an environment wrapper that starts the environment in a separate thread (or process if that's required) and communicates. doom_multiagent_wrapper.py is an example, although not optimal.","title":"Caveats"},{"location":"get-started/running-slurm/","text":"How to use Sample-Factory on Slurm \u00b6 This doc contains instructions for running Sample-Factory v2 using slurm Setting up \u00b6 Login to your slurm login node using ssh with your username and password. Start an interactive job with srun to install files to your NFS. srun -c40 --gres=gpu:1 --pty bash Note that you may get a message groups: cannot find name for group ID XXXX Install Miniconda - Download installer using wget from https://docs.conda.io/en/latest/miniconda.html#linux-installers - Run the installer with bash {Miniconda...sh} Make new conda environment conda create --name sf2 then conda activate sf2 Download Sample-Factory and install dependencies for Sample-Factory git clone https://github.com/alex-petrenko/sample-factory.git cd sample-factory git checkout sf2 pip install -e . Necessary scripts in Sample-Factory \u00b6 To run a custom launcher script for Sample-Factory on slurm, you may need to write your own slurm_sbatch_template and/or launcher script. slurm_sbatch_template is a bash script that run by slurm before your python script. It includes commands to activate your conda environment etc. See an example at ./sample_factory/launcher/slurm/sbatch_timeout.sh . Variables in the bash script can be added in sample_factory.launcher.run_slurm . The launcher script controls the python command slurm will run. Examples are located in sf_examples . You can run multiple experiments with different parameters using ParamGrid . Timeout Batch Script \u00b6 If your slurm cluster has time limits for jobs, you can use the sbatch_timeout.sh bash script to launch jobs that timeout and requeue themselves before the time limit. The time limit can be set with the slurm_timeout command line argument. It defaults to 0 which runs the job with no time limit. It is recommended the timeout be set to slightly less than the time limit of your job. For example, if the time limit is 24 hours, you should set --slurm_timeout=23h Running launcher scripts \u00b6 Return to the login node with exit Setup slurm output folder mkdir sf2 Activate your conda environment with bash and conda activate sf2 then cd sample-factory Run your launcher script - an example mujuco launcher (replace run, slurm_sbatch_template, and slurm_workdir with appropriate values) python -m sample_factory.launcher.run --backend=slurm --slurm_workdir=./slurm_mujoco --experiment_suffix=slurm --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=16 --slurm_sbatch_template=./sample_factory/launcher/slurm/sbatch_timeout.sh --pause_between=1 --slurm_print_only=False --run=sf_examples.mujoco.experiments.mujoco_all_envs The slurm_gpus_per_job and slurm_cpus_per_gpu determine the resources allocated to each job. You can view the jobs without running them by setting slurm_print_only=True . You can view the status of your jobs on nodes or the queue with squeue and view the outputs of your experiments with tail -f {slurm_workdir}/*.out . Cancel your jobs with scancel {job_id}","title":"How to use Sample-Factory on Slurm"},{"location":"get-started/running-slurm/#how-to-use-sample-factory-on-slurm","text":"This doc contains instructions for running Sample-Factory v2 using slurm","title":"How to use Sample-Factory on Slurm"},{"location":"get-started/running-slurm/#setting-up","text":"Login to your slurm login node using ssh with your username and password. Start an interactive job with srun to install files to your NFS. srun -c40 --gres=gpu:1 --pty bash Note that you may get a message groups: cannot find name for group ID XXXX Install Miniconda - Download installer using wget from https://docs.conda.io/en/latest/miniconda.html#linux-installers - Run the installer with bash {Miniconda...sh} Make new conda environment conda create --name sf2 then conda activate sf2 Download Sample-Factory and install dependencies for Sample-Factory git clone https://github.com/alex-petrenko/sample-factory.git cd sample-factory git checkout sf2 pip install -e .","title":"Setting up"},{"location":"get-started/running-slurm/#necessary-scripts-in-sample-factory","text":"To run a custom launcher script for Sample-Factory on slurm, you may need to write your own slurm_sbatch_template and/or launcher script. slurm_sbatch_template is a bash script that run by slurm before your python script. It includes commands to activate your conda environment etc. See an example at ./sample_factory/launcher/slurm/sbatch_timeout.sh . Variables in the bash script can be added in sample_factory.launcher.run_slurm . The launcher script controls the python command slurm will run. Examples are located in sf_examples . You can run multiple experiments with different parameters using ParamGrid .","title":"Necessary scripts in Sample-Factory"},{"location":"get-started/running-slurm/#timeout-batch-script","text":"If your slurm cluster has time limits for jobs, you can use the sbatch_timeout.sh bash script to launch jobs that timeout and requeue themselves before the time limit. The time limit can be set with the slurm_timeout command line argument. It defaults to 0 which runs the job with no time limit. It is recommended the timeout be set to slightly less than the time limit of your job. For example, if the time limit is 24 hours, you should set --slurm_timeout=23h","title":"Timeout Batch Script"},{"location":"get-started/running-slurm/#running-launcher-scripts","text":"Return to the login node with exit Setup slurm output folder mkdir sf2 Activate your conda environment with bash and conda activate sf2 then cd sample-factory Run your launcher script - an example mujuco launcher (replace run, slurm_sbatch_template, and slurm_workdir with appropriate values) python -m sample_factory.launcher.run --backend=slurm --slurm_workdir=./slurm_mujoco --experiment_suffix=slurm --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=16 --slurm_sbatch_template=./sample_factory/launcher/slurm/sbatch_timeout.sh --pause_between=1 --slurm_print_only=False --run=sf_examples.mujoco.experiments.mujoco_all_envs The slurm_gpus_per_job and slurm_cpus_per_gpu determine the resources allocated to each job. You can view the jobs without running them by setting slurm_print_only=True . You can view the status of your jobs on nodes or the queue with squeue and view the outputs of your experiments with tail -f {slurm_workdir}/*.out . Cancel your jobs with scancel {job_id}","title":"Running launcher scripts"},{"location":"release-notes/release-notes/","text":"Recent releases \u00b6 v2.0.0 \u00b6 Major update, adds new functionality, changes API and configuration parameters Major API update Synchronous and asynchronous training modes Serial and parallel execution modes Support for vectorized and GPU-accelerated environments in batched sampling mode Integration with Hugging Face Hub New environment integrations, CI, and documentation See v1 to v2 transition guide for details. v1.121.4 \u00b6 Support Weights and Biases (see section \"WandB support\") More configurable population-based training: can set from command line whether or not to mutate gamma, plus the perturbation magnitude for all float hyperparams can also be set from command line: --pbt_optimize_gamma: Whether to optimize gamma, discount factor, or not (experimental) (default: False) --pbt_perturb_min: When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.05) --pbt_perturb_max: When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.5) v1.121.3 \u00b6 Fixed a small bug related to population-based training (a reward shaping dictionary was assumed to be a flat dict, while it could be a nested dict in some envs) v1.121.2 \u00b6 Fixed a bug that prevented Vizdoom *.cfg and *.wad files from being copied to site-packages during installation from PyPI Added example on how to use custom Vizdoom envs without modifying the source code ( sample_factory_examples/train_custom_vizdoom_env.py ) v1.121.0 \u00b6 Added fixed KL divergence penalty as in https://arxiv.org/pdf/1707.06347.pdf Its usage is highly encouraged in environments with continuous action spaces (i.e. set --kl_loss_coeff=1.0). Otherwise numerical instabilities can occur in certain environments, especially when the policy lag is high More summaries related to the new loss v1.120.2 \u00b6 More improvements and fixes in runner interface, including support for NGC cluster v1.120.1 \u00b6 Runner interface improvements for Slurm v1.120.0 \u00b6 Support inactive agents. To deactivate an agent for a portion of the episode the environment should return info={'is_active': False} for the inactive agent. Useful for environments such as hide-n-seek. Improved memory consumption and performance with better shared memory management. Experiment logs are now saved into the experiment folder as sf_log.txt DMLab-related bug fixes (courtesy of @donghoonlee04 and @sungwoong . Thank you!)","title":"Recent releases"},{"location":"release-notes/release-notes/#recent-releases","text":"","title":"Recent releases"},{"location":"release-notes/release-notes/#v200","text":"Major update, adds new functionality, changes API and configuration parameters Major API update Synchronous and asynchronous training modes Serial and parallel execution modes Support for vectorized and GPU-accelerated environments in batched sampling mode Integration with Hugging Face Hub New environment integrations, CI, and documentation See v1 to v2 transition guide for details.","title":"v2.0.0"},{"location":"release-notes/release-notes/#v11214","text":"Support Weights and Biases (see section \"WandB support\") More configurable population-based training: can set from command line whether or not to mutate gamma, plus the perturbation magnitude for all float hyperparams can also be set from command line: --pbt_optimize_gamma: Whether to optimize gamma, discount factor, or not (experimental) (default: False) --pbt_perturb_min: When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.05) --pbt_perturb_max: When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.5)","title":"v1.121.4"},{"location":"release-notes/release-notes/#v11213","text":"Fixed a small bug related to population-based training (a reward shaping dictionary was assumed to be a flat dict, while it could be a nested dict in some envs)","title":"v1.121.3"},{"location":"release-notes/release-notes/#v11212","text":"Fixed a bug that prevented Vizdoom *.cfg and *.wad files from being copied to site-packages during installation from PyPI Added example on how to use custom Vizdoom envs without modifying the source code ( sample_factory_examples/train_custom_vizdoom_env.py )","title":"v1.121.2"},{"location":"release-notes/release-notes/#v11210","text":"Added fixed KL divergence penalty as in https://arxiv.org/pdf/1707.06347.pdf Its usage is highly encouraged in environments with continuous action spaces (i.e. set --kl_loss_coeff=1.0). Otherwise numerical instabilities can occur in certain environments, especially when the policy lag is high More summaries related to the new loss","title":"v1.121.0"},{"location":"release-notes/release-notes/#v11202","text":"More improvements and fixes in runner interface, including support for NGC cluster","title":"v1.120.2"},{"location":"release-notes/release-notes/#v11201","text":"Runner interface improvements for Slurm","title":"v1.120.1"},{"location":"release-notes/release-notes/#v11200","text":"Support inactive agents. To deactivate an agent for a portion of the episode the environment should return info={'is_active': False} for the inactive agent. Useful for environments such as hide-n-seek. Improved memory consumption and performance with better shared memory management. Experiment logs are now saved into the experiment folder as sf_log.txt DMLab-related bug fixes (courtesy of @donghoonlee04 and @sungwoong . Thank you!)","title":"v1.120.0"}]}